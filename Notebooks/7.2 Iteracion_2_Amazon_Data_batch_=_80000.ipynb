{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-SSaV-bRZZl"
   },
   "source": [
    "# Paso 1: Importar Librerías de Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K4mFYevMKu6y"
   },
   "outputs": [],
   "source": [
    "#pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "47ItRxlORZ9i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from surprise import SVD, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, Dot\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bHcBRlQYY4h",
    "outputId": "873211b6-dc55-4eed-e23c-ca1e4b3ba68d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "209f_YHQRaCi"
   },
   "source": [
    "# Paso 2: Lectura del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vIFqmMIRaIc",
    "outputId": "d4cb05cd-bca9-4ea4-d622-971b54eb6fb7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-1f8e3361d82a>:2: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')  # Descomenta y proporciona la ruta si estás cargando un archivo CSV\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gMMpglLyszaX"
   },
   "outputs": [],
   "source": [
    "# Eliminemos posibles duplicados:\n",
    "df = df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwnA5HnLr8r6",
    "outputId": "55168388-92ec-4398-9e1b-aa327ba998f5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4641903, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxZN3FMBKeKK"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando SVD\n",
    "\n",
    "Utilizaremos SVD para hacer recomendaciones basadas en interacciones pasadas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XrvAbzJn7RK"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "SVD es una técnica matemática que descompone una matriz en tres matrices más pequeñas: U, Σ y V*. En el contexto de sistemas de recomendación, se usa para factorizar la matriz de usuario-ítem en componentes latentes, capturando patrones subyacentes en los datos.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Es matemáticamente robusto y ha sido una técnica establecida durante mucho tiempo.\n",
    "\n",
    "Puede capturar relaciones no evidentes en los datos.\n",
    "\n",
    "Reducción de dimensionalidad: al capturar la esencia de los datos en factores latentes, se puede trabajar con dimensiones reducidas.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "No maneja bien datos faltantes. La matriz de usuario-ítem suele ser dispersa, y el SVD estándar no se diseñó para manejar matrices con muchos valores faltantes.\n",
    "\n",
    "Puede ser computacionalmente costoso para matrices muy grandes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8EsHgJSOI-8",
    "outputId": "17d2ca92-0764-47c9-d37a-3f487de2b949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.0864\n",
      "SVD - RMSE: 1.0864052122296506, MSE: 1.180276285159752, MAE: 0.8192398597949374, MAPE: 33.94576673547577\n"
     ]
    }
   ],
   "source": [
    "# Leer y procesar datos\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df[['reviewerID', 'asin', 'overall']], reader)\n",
    "\n",
    "# Dividir el dataset\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.3, random_state=10)\n",
    "\n",
    "# Entrenar el modelo SVD\n",
    "svd = SVD()\n",
    "svd.fit(trainset)\n",
    "predictions_svd = svd.test(testset)\n",
    "\n",
    "# Función para calcular el Mean Absolute Percentage Error (MAPE)\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Evaluar el modelo\n",
    "rmse_svd = accuracy.rmse(predictions_svd, verbose=True)\n",
    "mse_svd = mean_squared_error([pred.r_ui for pred in predictions_svd], [pred.est for pred in predictions_svd])\n",
    "mae_svd = mean_absolute_error([pred.r_ui for pred in predictions_svd], [pred.est for pred in predictions_svd])\n",
    "mape_svd = mape([pred.r_ui for pred in predictions_svd], [pred.est for pred in predictions_svd])\n",
    "\n",
    "print(f\"SVD - RMSE: {rmse_svd}, MSE: {mse_svd}, MAE: {mae_svd}, MAPE: {mape_svd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lMsQcyyS2NS"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Embedding con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ49x7CvmOfM"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Los embeddings son representaciones vectoriales densas y de baja dimensión de ítems y/o usuarios. Estas representaciones capturan relaciones semánticas entre ítems o entre usuarios.\n",
    "\n",
    "Un método popular para generar embeddings es la factorización de matrices, como la descomposición en valores singulares (SVD). En el contexto de sistemas de recomendación, se busca factorizar la matriz de interacciones usuario-ítem en dos matrices más pequeñas (una para los usuarios y otra para los ítems) cuyo producto aproximado reproduce la matriz original lo mejor posible.\n",
    "\n",
    "Una vez que se han obtenido los embeddings, la predicción de una calificación o interacción entre un usuario e ítem se realiza tomando el producto escalar entre sus embeddings respectivos.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Simplicidad y eficiencia en términos computacionales.\n",
    "\n",
    "Puede manejar grandes conjuntos de datos debido a su naturaleza de baja dimensión.\n",
    "\n",
    "Es efectivo para capturar patrones subyacentes en los datos.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "No tiene en cuenta características adicionales de usuarios o ítems.\n",
    "\n",
    "Dificultades para manejar nuevos ítems o usuarios (problema de arranque en frío)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oQAn6BZT8ZB"
   },
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22AEmdhqU9vC"
   },
   "outputs": [],
   "source": [
    "# Crear un LabelEncoder para cada columna\n",
    "reviewerID_encoder = LabelEncoder()\n",
    "asin_encoder = LabelEncoder()\n",
    "\n",
    "# Ajustar y transformar las columnas\n",
    "data['reviewerID_encoded'] = reviewerID_encoder.fit_transform(data['reviewerID'])\n",
    "data['asin_encoded'] = asin_encoder.fit_transform(data['asin'])\n",
    "\n",
    "# Mostrar las primeras filas del DataFrame para verificar\n",
    "#print(data[['reviewerID', 'reviewerID_encoded', 'asin', 'asin_encoded']].head())\n",
    "\n",
    "\n",
    "# Número de usuarios únicos y número de ítems únicos\n",
    "n_users = len(np.unique(data['reviewerID_encoded']))\n",
    "n_items = len(np.unique(data['asin_encoded']))\n",
    "\n",
    "# Dimensiones del embedding\n",
    "embedding_dim = 10\n",
    "\n",
    "# Entradas\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "\n",
    "# Embeddings\n",
    "user_embedding = Embedding(n_users, embedding_dim)(user_input)\n",
    "item_embedding = Embedding(n_items, embedding_dim)(item_input)\n",
    "\n",
    "# Producto punto para predecir la valoración/rating\n",
    "merged = Dot(axes=2)([user_embedding, item_embedding])\n",
    "merged = Flatten()(merged)\n",
    "\n",
    "# Modelo\n",
    "model = Model(inputs=[user_input, item_input], outputs=merged)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN3t1PGSYEzR"
   },
   "source": [
    "verbose=0: No muestra ninguna barra de progreso ni métricas.\n",
    "\n",
    "verbose=1: Muestra una barra de progreso y actualiza las métricas después de cada lote.\n",
    "\n",
    "verbose=2: Muestra las métricas después de cada época, pero no muestra la barra de progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3n02-vPg5wh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Crear los arrays con los datos\n",
    "user_ids = data['reviewerID_encoded'].values\n",
    "item_ids = data['asin_encoded'].values\n",
    "ratings = data['overall'].values\n",
    "\n",
    "# Dividir en train y test\n",
    "(user_ids_train, user_ids_test, item_ids_train, item_ids_test, ratings_train, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOiSfVMfg5tw",
    "outputId": "f7280cc9-1750-4d1a-a80d-54038ff8889a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 11s 231ms/step - loss: 19.4976 - mae: 4.2587\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 8s 201ms/step - loss: 19.4897 - mae: 4.2578\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 7s 180ms/step - loss: 19.4762 - mae: 4.2563\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 8s 189ms/step - loss: 19.4483 - mae: 4.2532\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 8s 192ms/step - loss: 19.3959 - mae: 4.2472\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 6s 144ms/step - loss: 19.3089 - mae: 4.2373\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 5s 113ms/step - loss: 19.1793 - mae: 4.2224\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 5s 119ms/step - loss: 19.0024 - mae: 4.2017\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 5s 108ms/step - loss: 18.7768 - mae: 4.1750\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 4s 94ms/step - loss: 18.5035 - mae: 4.1422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7abd637966e0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "model.fit([user_ids_train, item_ids_train], ratings_train, epochs=10, batch_size=80000, verbose=1)\n",
    "\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMJd_nt7g5h4",
    "outputId": "2c32bd12-7a9e-4375-9693-25d55eca3a25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 2ms/step - loss: 19.2073 - mae: 4.2225\n",
      "Test Loss: 19.2073\n",
      "Test MAE: 4.2225\n"
     ]
    }
   ],
   "source": [
    "# Evaluar el modelo con el conjunto de prueba\n",
    "loss, mae = model.evaluate([user_ids_test, item_ids_test], ratings_test, batch_size=80000)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USnKRkm0KCdW",
    "outputId": "0403a367-b7c0-42d7-c4bd-4f1ec91bd7fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 19.2073\n",
      "RMSE: 4.3826\n",
      "MAE: 4.2225\n",
      "MAPE: 99.04%\n"
     ]
    }
   ],
   "source": [
    "# MSE\n",
    "mse = mean_squared_error(ratings_test, predictions)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(ratings_test, predictions)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((ratings_test - predictions.flatten()) / ratings_test)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odgbIWArkJ2y"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBK_zPK7lvti"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Estos sistemas toman características de los ítems y/o usuarios y las pasan a través de una o varias capas de neuronas para obtener una predicción.\n",
    "Las redes neuronales son capaces de capturar interacciones no lineales entre características, lo que las hace poderosas para tareas de modelado complejas.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de modelar relaciones no lineales.\n",
    "\n",
    "Flexibilidad para incorporar múltiples fuentes de datos o características.\n",
    "\n",
    "Puede manejar arranques en frío al incorporar características de nuevos ítems o usuarios.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor costo computacional en comparación con los sistemas basados únicamente en embeddings.\n",
    "\n",
    "Riesgo de sobreajuste si no se tiene un conjunto de datos lo suficientemente grande."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFgVZSeTkKGy"
   },
   "outputs": [],
   "source": [
    "# Convertir reviewerID y asin a índices numéricos secuenciales\n",
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# Número de usuarios e ítems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# Hiperparámetros\n",
    "hidden_units = [128, 64, 32]  # Unidades en las capas ocultas\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Arquitectura del modelo\n",
    "user_input = Input(shape=[1], name='user_input')\n",
    "item_input = Input(shape=[1], name='item_input')\n",
    "concat = Concatenate()([user_input, item_input])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu')(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1)(dense)\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blU4Q9akkVBc",
    "outputId": "5f450ae4-b048-4e32-92c4-e392e7937aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 3s 18ms/step - loss: 1135308928.0000 - val_loss: 14672278.0000\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 73755912.0000 - val_loss: 782145.6875\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 12576571.0000 - val_loss: 34165.4375\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 3624744.7500 - val_loss: 6065.4355\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 1757924.8750 - val_loss: 1307.5425\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 1131483.8750 - val_loss: 477.2494\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 835641.3125 - val_loss: 290.3395\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 650706.6875 - val_loss: 168.8172\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 521061.7188 - val_loss: 99.4635\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 1s 13ms/step - loss: 441513.6875 - val_loss: 81.0033\n",
      "43518/43518 [==============================] - 59s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# Usando validation_split, divide los datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_size = int(0.7 * len(user_data))\n",
    "user_data_train, user_data_val = user_data[:train_size], user_data[train_size:]\n",
    "item_data_train, item_data_val = item_data[:train_size], item_data[train_size:]\n",
    "y_true = rating_data[train_size:]\n",
    "\n",
    "history = model.fit([user_data_train, item_data_train], rating_data[:train_size],\n",
    "                    epochs=10, validation_data=([user_data_val, item_data_val], y_true),\n",
    "                    batch_size=80000, verbose=1)\n",
    "\n",
    "# Predicciones\n",
    "predictions = model.predict([user_data_val, item_data_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmYiE_PhkU66",
    "outputId": "768bc068-6595-4dae-f3bc-7295661d51b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 81.00335522250023\n",
      "RMSE: 9.000186399319752\n",
      "MAE: 8.57458161361687\n",
      "MAPE: 223.03449762080908%\n"
     ]
    }
   ],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# Funciones MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la función\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTOAZeGrujrU"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa y Embedding con Keras:\n",
    "\n",
    "Este enfoque, que combina embeddings y redes neuronales multicapa, puede capturar interacciones más complejas y no lineales entre usuarios y artículos. Sin embargo, es crucial prestar atención al sobreajuste y asegurarse de que el modelo no esté simplemente memorizando los datos. Por lo tanto, es recomendable emplear técnicas de regularización, ajustar hiperparámetros y validar el rendimiento con un conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Rzzhtfl7nK"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Combina lo mejor de ambos mundos. Primero, se utilizan embeddings para convertir ítems y usuarios en representaciones vectoriales densas. Luego, estas representaciones se pasan a través de una red neuronal para hacer la predicción.\n",
    "\n",
    "El proceso generalmente comienza con capas de embedding que convierten identificadores de usuarios e ítems en vectores. Estos vectores luego se pasan a través de capas densas para obtener la predicción final.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de capturar patrones subyacentes en los datos mediante embeddings y modelar interacciones no lineales mediante la red neuronal.\n",
    "\n",
    "Flexibilidad para incorporar características adicionales.\n",
    "\n",
    "Potencialmente más preciso que cualquiera de los otros dos métodos por separado.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor complejidad en el modelado y entrenamiento.\n",
    "\n",
    "Mayor costo computacional.\n",
    "\n",
    "Requiere un ajuste más cuidadoso y riesgo de sobreajuste si no se gestiona adecuadamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVRB5utBuqzB"
   },
   "outputs": [],
   "source": [
    "# Convertir reviewerID y asin a índices numéricos secuenciales\n",
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# Número de usuarios e ítems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# Hiperparámetros\n",
    "n_latent_factors = 50  # Número de factores latentes\n",
    "hidden_units = [128, 64]  # Unidades en las capas ocultas\n",
    "dropout_rate = 0.2\n",
    "\n",
    "# Arquitectura del modelo\n",
    "user_input = Input(shape=[1], name='user_input')\n",
    "item_input = Input(shape=[1], name='item_input')\n",
    "user_embedding = Embedding(n_users, n_latent_factors, name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_items, n_latent_factors, name='item_embedding')(item_input)\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "item_vec = Flatten(name='flatten_items')(item_embedding)\n",
    "concat = Concatenate()([user_vec, item_vec])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu')(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1)(dense)\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XJfWiirYxHwk",
    "outputId": "e544afd2-1bc7-4992-caa8-f23ba157eaac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "41/41 [==============================] - 13s 290ms/step - loss: 13.9407 - val_loss: 8.0267\n",
      "Epoch 2/10\n",
      "41/41 [==============================] - 11s 256ms/step - loss: 1.8954 - val_loss: 5.2937\n",
      "Epoch 3/10\n",
      "41/41 [==============================] - 9s 222ms/step - loss: 1.1846 - val_loss: 4.9302\n",
      "Epoch 4/10\n",
      "41/41 [==============================] - 9s 219ms/step - loss: 0.9862 - val_loss: 4.7563\n",
      "Epoch 5/10\n",
      "41/41 [==============================] - 9s 232ms/step - loss: 0.9110 - val_loss: 4.6494\n",
      "Epoch 6/10\n",
      "41/41 [==============================] - 7s 166ms/step - loss: 0.8733 - val_loss: 4.5646\n",
      "Epoch 7/10\n",
      "41/41 [==============================] - 7s 177ms/step - loss: 0.8489 - val_loss: 4.4939\n",
      "Epoch 8/10\n",
      "41/41 [==============================] - 6s 149ms/step - loss: 0.8289 - val_loss: 4.4362\n",
      "Epoch 9/10\n",
      "41/41 [==============================] - 6s 147ms/step - loss: 0.8122 - val_loss: 4.3828\n",
      "Epoch 10/10\n",
      "41/41 [==============================] - 6s 128ms/step - loss: 0.7985 - val_loss: 4.2869\n",
      "43518/43518 [==============================] - 60s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# Entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# Usando validation_split, divide los datos\n",
    "train_size = int(0.7 * len(user_data))\n",
    "user_data_train, user_data_val = user_data[:train_size], user_data[train_size:]\n",
    "item_data_train, item_data_val = item_data[:train_size], item_data[train_size:]\n",
    "y_true = rating_data[train_size:]\n",
    "\n",
    "history = model.fit([user_data_train, item_data_train], rating_data[:train_size],\n",
    "                    epochs=10, validation_data=([user_data_val, item_data_val], y_true),\n",
    "                    batch_size=80000, verbose=1)\n",
    "\n",
    "# Predicciones\n",
    "predictions = model.predict([user_data_val, item_data_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1BY-Sd0xJtc",
    "outputId": "f733d35e-e665-46e0-b1cb-a7fc99d7b1cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 4.286940096338541\n",
      "RMSE: 2.070492718252962\n",
      "MAE: 1.8667582920725043\n",
      "MAPE: 49.114634109069634%\n"
     ]
    }
   ],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la función\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YopA3D2Sw8AO"
   },
   "source": [
    "Estas métricas te ayudarán a tener una mejor idea del desempeño de tu modelo. Por ejemplo:\n",
    "\n",
    "MSE y RMSE son útiles cuando quieres penalizar grandes errores.\n",
    "\n",
    "MAE te da una idea del error medio sin considerar la dirección del error.\n",
    "\n",
    "MAPE es útil cuando quieres representar el error en términos porcentuales.\n",
    "\n",
    "Para una evaluación completa, es recomendable utilizar un conjunto de validación aparte (es decir, no solo depender del validation_split). Esto asegura que estás evaluando el desempeño en datos que el modelo nunca ha visto durante el entrenamiento."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
