{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-SSaV-bRZZl"
      },
      "source": [
        "# Paso 1: Importar Librerías de Python\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K4mFYevMKu6y",
        "outputId": "44990245-5ba0-4c54-95d5-e698619b668a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-surprise\n",
            "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m772.0/772.0 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.3.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-surprise) (1.11.3)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.3-cp310-cp310-linux_x86_64.whl size=3163490 sha256=2bc40e1ee6d6fc15b77eb5289a809c020ede91026b9c2e5fc217586b5c01f8dc\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/ca/a8/4e28def53797fdc4363ca4af740db15a9c2f1595ebc51fb445\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.3\n"
          ]
        }
      ],
      "source": [
        "#pip install scikit-surprise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "47ItRxlORZ9i"
      },
      "outputs": [],
      "source": [
        "# Análisis de datos y manipulación\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualización de datos\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Utilidades de fechas\n",
        "from datetime import datetime\n",
        "\n",
        "# Integración con Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Machine Learning: Preprocesamiento y división de datos\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Machine Learning: Métricas\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Machine Learning: Modelado y validación\n",
        "from surprise import SVD, Reader, Dataset, accuracy\n",
        "from surprise.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "# Deep Learning: Construcción de modelos y capas\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, Dot\n",
        "\n",
        "# Deep Learning: Optimización y regularización\n",
        "from keras.optimizers import Adam\n",
        "from keras.regularizers import l2\n",
        "\n",
        "# Deep Learning: Callbacks\n",
        "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bHcBRlQYY4h",
        "outputId": "3cfe8214-3ad7-45f6-e379-4157546f5139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "209f_YHQRaCi"
      },
      "source": [
        "# Paso 2: Lectura del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vIFqmMIRaIc",
        "outputId": "8298afbf-5e38-4e2c-a036-d7107d1fb2ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-6d023f7e94e1>:1: DtypeWarning: Columns (5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')  # Descomenta y proporciona la ruta si estás cargando un archivo CSV\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')  # Descomenta y proporciona la ruta si estás cargando un archivo CSV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "yBhdLFP8Raza"
      },
      "outputs": [],
      "source": [
        "# Eliminemos posibles duplicados:\n",
        "df = df.drop_duplicates()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LwnA5HnLr8r6",
        "outputId": "c92afcf2-547a-4288-f5a8-4d6019c24454"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4641903, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lMsQcyyS2NS"
      },
      "source": [
        "# Modelo de Filtrado Colaborativo usando Embedding con Keras:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ49x7CvmOfM"
      },
      "source": [
        "Funcionamiento:\n",
        "\n",
        "Los embeddings son representaciones vectoriales densas y de baja dimensión de ítems y/o usuarios. Estas representaciones capturan relaciones semánticas entre ítems o entre usuarios.\n",
        "\n",
        "Un método popular para generar embeddings es la factorización de matrices, como la descomposición en valores singulares (SVD). En el contexto de sistemas de recomendación, se busca factorizar la matriz de interacciones usuario-ítem en dos matrices más pequeñas (una para los usuarios y otra para los ítems) cuyo producto aproximado reproduce la matriz original lo mejor posible.\n",
        "\n",
        "Una vez que se han obtenido los embeddings, la predicción de una calificación o interacción entre un usuario e ítem se realiza tomando el producto escalar entre sus embeddings respectivos.\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "Simplicidad y eficiencia en términos computacionales.\n",
        "\n",
        "Puede manejar grandes conjuntos de datos debido a su naturaleza de baja dimensión.\n",
        "\n",
        "Es efectivo para capturar patrones subyacentes en los datos.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "No tiene en cuenta características adicionales de usuarios o ítems.\n",
        "\n",
        "Dificultades para manejar nuevos ítems o usuarios (problema de arranque en frío)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJ482XODiA8T"
      },
      "source": [
        "El código ahora incluye regularización en los embeddings, un callback para detener el entrenamiento si no hay mejoras (early stopping) y otro para reducir la tasa de aprendizaje si el error de validación no mejora (reduce learning rate on plateau)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "3oQAn6BZT8ZB"
      },
      "outputs": [],
      "source": [
        "data = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "22AEmdhqU9vC"
      },
      "outputs": [],
      "source": [
        "# Crear un LabelEncoder para cada columna\n",
        "reviewerID_encoder = LabelEncoder()\n",
        "asin_encoder = LabelEncoder()\n",
        "\n",
        "# Ajustar y transformar las columnas\n",
        "data['reviewerID_encoded'] = reviewerID_encoder.fit_transform(data['reviewerID'])\n",
        "data['asin_encoded'] = asin_encoder.fit_transform(data['asin'])\n",
        "\n",
        "# Número de usuarios únicos y número de ítems únicos\n",
        "n_users = len(np.unique(data['reviewerID_encoded']))\n",
        "n_items = len(np.unique(data['asin_encoded']))\n",
        "\n",
        "# Dimensiones del embedding\n",
        "embedding_dim = 10\n",
        "\n",
        "# Entradas\n",
        "user_input = Input(shape=(1,))\n",
        "item_input = Input(shape=(1,))\n",
        "\n",
        "# Embeddings con regularización\n",
        "user_embedding = Embedding(n_users, embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)\n",
        "item_embedding = Embedding(n_items, embedding_dim, embeddings_regularizer=l2(1e-6))(item_input)\n",
        "\n",
        "# Producto punto para predecir la valoración/rating\n",
        "merged = Dot(axes=2)([user_embedding, item_embedding])\n",
        "merged = Flatten()(merged)\n",
        "\n",
        "# Modelo\n",
        "model = Model(inputs=[user_input, item_input], outputs=merged)\n",
        "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN3t1PGSYEzR"
      },
      "source": [
        "verbose=0: No muestra ninguna barra de progreso ni métricas.\n",
        "\n",
        "verbose=1: Muestra una barra de progreso y actualiza las métricas después de cada lote.\n",
        "\n",
        "verbose=2: Muestra las métricas después de cada época, pero no muestra la barra de progreso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "J3n02-vPg5wh"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Crear los arrays con los datos\n",
        "user_ids = data['reviewerID_encoded'].values\n",
        "item_ids = data['asin_encoded'].values\n",
        "ratings = data['overall'].values\n",
        "\n",
        "# Dividir en train y test\n",
        "(user_ids_train, user_ids_test, item_ids_train, item_ids_test, ratings_train, ratings_test) = train_test_split(\n",
        "    user_ids, item_ids, ratings,test_size=0.3,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "NYbYJfevgXpj"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOiSfVMfg5tw",
        "outputId": "3371109d-45a1-404e-e9d8-47ff83f623d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "82/82 [==============================] - 6s 12ms/step - loss: 19.5093 - mae: 4.2587 - val_loss: 19.5115 - val_mae: 4.2592 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 19.4934 - mae: 4.2572 - val_loss: 19.5033 - val_mae: 4.2584 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 19.4565 - mae: 4.2529 - val_loss: 19.4787 - val_mae: 4.2552 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 19.3695 - mae: 4.2424 - val_loss: 19.4318 - val_mae: 4.2488 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 19.2084 - mae: 4.2225 - val_loss: 19.3651 - val_mae: 4.2389 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 18.9613 - mae: 4.1911 - val_loss: 19.2845 - val_mae: 4.2260 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 18.6282 - mae: 4.1477 - val_loss: 19.1961 - val_mae: 4.2103 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 18.2189 - mae: 4.0928 - val_loss: 19.1062 - val_mae: 4.1926 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 17.7475 - mae: 4.0274 - val_loss: 19.0201 - val_mae: 4.1733 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 17.2293 - mae: 3.9530 - val_loss: 18.9413 - val_mae: 4.1530 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 16.6790 - mae: 3.8711 - val_loss: 18.8722 - val_mae: 4.1323 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 16.1092 - mae: 3.7832 - val_loss: 18.8141 - val_mae: 4.1114 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 15.5309 - mae: 3.6907 - val_loss: 18.7668 - val_mae: 4.0909 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 14.9529 - mae: 3.5948 - val_loss: 18.7301 - val_mae: 4.0707 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 14.3825 - mae: 3.4966 - val_loss: 18.7035 - val_mae: 4.0511 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 13.8256 - mae: 3.3971 - val_loss: 18.6860 - val_mae: 4.0321 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 13.2867 - mae: 3.2972 - val_loss: 18.6766 - val_mae: 4.0138 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 12.7691 - mae: 3.1975 - val_loss: 18.6744 - val_mae: 3.9960 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 12.2749 - mae: 3.0988 - val_loss: 18.6785 - val_mae: 3.9788 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 11.8056 - mae: 3.0016 - val_loss: 18.6876 - val_mae: 3.9621 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 11.3614 - mae: 2.9065 - val_loss: 18.7010 - val_mae: 3.9459 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 10.9423 - mae: 2.8139 - val_loss: 18.7174 - val_mae: 3.9302 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 10.5476 - mae: 2.7240 - val_loss: 18.7363 - val_mae: 3.9149 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 10.1763 - mae: 2.6373 - val_loss: 18.7566 - val_mae: 3.9000 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 9.8273 - mae: 2.5537 - val_loss: 18.7778 - val_mae: 3.8855 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 9.4993 - mae: 2.4735 - val_loss: 18.7991 - val_mae: 3.8713 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 9.1910 - mae: 2.3965 - val_loss: 18.8201 - val_mae: 3.8574 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 8.9010 - mae: 2.3228 - val_loss: 18.8404 - val_mae: 3.8438 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 8.6282 - mae: 2.2523 - val_loss: 18.8597 - val_mae: 3.8305 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 8.3714 - mae: 2.1848 - val_loss: 18.8776 - val_mae: 3.8174 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 8.1293 - mae: 2.1203 - val_loss: 18.8941 - val_mae: 3.8045 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 7.9010 - mae: 2.0586 - val_loss: 18.9089 - val_mae: 3.7919 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 7.6856 - mae: 1.9996 - val_loss: 18.9220 - val_mae: 3.7795 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 7.4823 - mae: 1.9431 - val_loss: 18.9335 - val_mae: 3.7673 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 7.2902 - mae: 1.8890 - val_loss: 18.9433 - val_mae: 3.7553 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 7.1086 - mae: 1.8372 - val_loss: 18.9511 - val_mae: 3.7435 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.9369 - mae: 1.7874 - val_loss: 18.9573 - val_mae: 3.7318 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.7744 - mae: 1.7398 - val_loss: 18.9620 - val_mae: 3.7204 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.6206 - mae: 1.6940 - val_loss: 18.9648 - val_mae: 3.7091 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.4750 - mae: 1.6501 - val_loss: 18.9660 - val_mae: 3.6980 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.3372 - mae: 1.6079 - val_loss: 18.9658 - val_mae: 3.6870 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.2065 - mae: 1.5673 - val_loss: 18.9638 - val_mae: 3.6762 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 6.0827 - mae: 1.5283 - val_loss: 18.9605 - val_mae: 3.6655 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.9653 - mae: 1.4908 - val_loss: 18.9556 - val_mae: 3.6550 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.8541 - mae: 1.4547 - val_loss: 18.9494 - val_mae: 3.6446 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.7485 - mae: 1.4200 - val_loss: 18.9418 - val_mae: 3.6343 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.6485 - mae: 1.3865 - val_loss: 18.9329 - val_mae: 3.6242 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.5535 - mae: 1.3544 - val_loss: 18.9228 - val_mae: 3.6143 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.4635 - mae: 1.3234 - val_loss: 18.9114 - val_mae: 3.6044 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.3780 - mae: 1.2935 - val_loss: 18.8986 - val_mae: 3.5947 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.2968 - mae: 1.2647 - val_loss: 18.8848 - val_mae: 3.5851 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.2198 - mae: 1.2371 - val_loss: 18.8699 - val_mae: 3.5756 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.1467 - mae: 1.2103 - val_loss: 18.8540 - val_mae: 3.5663 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.0773 - mae: 1.1846 - val_loss: 18.8367 - val_mae: 3.5570 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 5.0114 - mae: 1.1598 - val_loss: 18.8184 - val_mae: 3.5479 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.9487 - mae: 1.1359 - val_loss: 18.7993 - val_mae: 3.5388 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.8892 - mae: 1.1129 - val_loss: 18.7790 - val_mae: 3.5299 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.8327 - mae: 1.0906 - val_loss: 18.7578 - val_mae: 3.5211 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.7789 - mae: 1.0692 - val_loss: 18.7355 - val_mae: 3.5123 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.7279 - mae: 1.0486 - val_loss: 18.7123 - val_mae: 3.5037 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.6793 - mae: 1.0287 - val_loss: 18.6881 - val_mae: 3.4951 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.6331 - mae: 1.0095 - val_loss: 18.6630 - val_mae: 3.4866 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.5892 - mae: 0.9910 - val_loss: 18.6371 - val_mae: 3.4782 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.5474 - mae: 0.9732 - val_loss: 18.6103 - val_mae: 3.4699 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.5076 - mae: 0.9560 - val_loss: 18.5827 - val_mae: 3.4617 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.4698 - mae: 0.9394 - val_loss: 18.5541 - val_mae: 3.4535 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.4337 - mae: 0.9234 - val_loss: 18.5249 - val_mae: 3.4454 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.3995 - mae: 0.9080 - val_loss: 18.4946 - val_mae: 3.4373 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.3667 - mae: 0.8932 - val_loss: 18.4638 - val_mae: 3.4294 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.3356 - mae: 0.8789 - val_loss: 18.4321 - val_mae: 3.4215 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.3059 - mae: 0.8651 - val_loss: 18.3995 - val_mae: 3.4136 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.2776 - mae: 0.8518 - val_loss: 18.3662 - val_mae: 3.4057 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.2505 - mae: 0.8389 - val_loss: 18.3324 - val_mae: 3.3980 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.2247 - mae: 0.8265 - val_loss: 18.2978 - val_mae: 3.3903 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.2001 - mae: 0.8146 - val_loss: 18.2623 - val_mae: 3.3826 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.1766 - mae: 0.8031 - val_loss: 18.2263 - val_mae: 3.3750 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.1542 - mae: 0.7921 - val_loss: 18.1895 - val_mae: 3.3674 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.1327 - mae: 0.7814 - val_loss: 18.1522 - val_mae: 3.3599 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.1122 - mae: 0.7710 - val_loss: 18.1142 - val_mae: 3.3524 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0926 - mae: 0.7612 - val_loss: 18.0755 - val_mae: 3.3449 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0738 - mae: 0.7516 - val_loss: 18.0362 - val_mae: 3.3375 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0557 - mae: 0.7424 - val_loss: 17.9964 - val_mae: 3.3300 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0385 - mae: 0.7335 - val_loss: 17.9560 - val_mae: 3.3227 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0219 - mae: 0.7249 - val_loss: 17.9150 - val_mae: 3.3153 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 4.0061 - mae: 0.7167 - val_loss: 17.8735 - val_mae: 3.3080 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9908 - mae: 0.7087 - val_loss: 17.8314 - val_mae: 3.3007 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9762 - mae: 0.7010 - val_loss: 17.7887 - val_mae: 3.2934 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9621 - mae: 0.6936 - val_loss: 17.7458 - val_mae: 3.2862 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9485 - mae: 0.6865 - val_loss: 17.7020 - val_mae: 3.2789 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9355 - mae: 0.6796 - val_loss: 17.6579 - val_mae: 3.2717 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9230 - mae: 0.6729 - val_loss: 17.6134 - val_mae: 3.2645 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.9109 - mae: 0.6666 - val_loss: 17.5683 - val_mae: 3.2573 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8992 - mae: 0.6603 - val_loss: 17.5228 - val_mae: 3.2501 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8880 - mae: 0.6544 - val_loss: 17.4769 - val_mae: 3.2430 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8771 - mae: 0.6487 - val_loss: 17.4305 - val_mae: 3.2358 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8666 - mae: 0.6431 - val_loss: 17.3839 - val_mae: 3.2287 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8565 - mae: 0.6378 - val_loss: 17.3368 - val_mae: 3.2215 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8466 - mae: 0.6326 - val_loss: 17.2895 - val_mae: 3.2144 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8371 - mae: 0.6276 - val_loss: 17.2417 - val_mae: 3.2073 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8279 - mae: 0.6228 - val_loss: 17.1936 - val_mae: 3.2002 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8189 - mae: 0.6182 - val_loss: 17.1451 - val_mae: 3.1931 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8103 - mae: 0.6137 - val_loss: 17.0962 - val_mae: 3.1860 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.8018 - mae: 0.6094 - val_loss: 17.0472 - val_mae: 3.1790 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7936 - mae: 0.6052 - val_loss: 16.9978 - val_mae: 3.1719 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7857 - mae: 0.6012 - val_loss: 16.9481 - val_mae: 3.1648 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7779 - mae: 0.5973 - val_loss: 16.8982 - val_mae: 3.1578 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7704 - mae: 0.5936 - val_loss: 16.8481 - val_mae: 3.1507 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7630 - mae: 0.5899 - val_loss: 16.7976 - val_mae: 3.1436 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7559 - mae: 0.5864 - val_loss: 16.7470 - val_mae: 3.1366 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7489 - mae: 0.5830 - val_loss: 16.6960 - val_mae: 3.1295 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7421 - mae: 0.5798 - val_loss: 16.6451 - val_mae: 3.1225 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7354 - mae: 0.5766 - val_loss: 16.5937 - val_mae: 3.1154 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7289 - mae: 0.5735 - val_loss: 16.5422 - val_mae: 3.1084 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7225 - mae: 0.5706 - val_loss: 16.4906 - val_mae: 3.1013 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7163 - mae: 0.5677 - val_loss: 16.4386 - val_mae: 3.0943 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7101 - mae: 0.5649 - val_loss: 16.3867 - val_mae: 3.0872 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.7042 - mae: 0.5622 - val_loss: 16.3348 - val_mae: 3.0802 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6983 - mae: 0.5596 - val_loss: 16.2825 - val_mae: 3.0731 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6926 - mae: 0.5571 - val_loss: 16.2302 - val_mae: 3.0661 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6869 - mae: 0.5547 - val_loss: 16.1776 - val_mae: 3.0590 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6814 - mae: 0.5524 - val_loss: 16.1251 - val_mae: 3.0520 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6760 - mae: 0.5501 - val_loss: 16.0723 - val_mae: 3.0449 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6706 - mae: 0.5479 - val_loss: 16.0198 - val_mae: 3.0379 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6654 - mae: 0.5458 - val_loss: 15.9669 - val_mae: 3.0309 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6602 - mae: 0.5436 - val_loss: 15.9141 - val_mae: 3.0238 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6551 - mae: 0.5416 - val_loss: 15.8613 - val_mae: 3.0168 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6501 - mae: 0.5397 - val_loss: 15.8085 - val_mae: 3.0098 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6452 - mae: 0.5378 - val_loss: 15.7555 - val_mae: 3.0027 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6404 - mae: 0.5359 - val_loss: 15.7025 - val_mae: 2.9957 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6356 - mae: 0.5342 - val_loss: 15.6497 - val_mae: 2.9887 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6309 - mae: 0.5325 - val_loss: 15.5968 - val_mae: 2.9817 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6263 - mae: 0.5308 - val_loss: 15.5437 - val_mae: 2.9747 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6217 - mae: 0.5291 - val_loss: 15.4908 - val_mae: 2.9677 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6172 - mae: 0.5276 - val_loss: 15.4379 - val_mae: 2.9606 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6128 - mae: 0.5260 - val_loss: 15.3850 - val_mae: 2.9537 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6084 - mae: 0.5246 - val_loss: 15.3323 - val_mae: 2.9467 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.6041 - mae: 0.5231 - val_loss: 15.2794 - val_mae: 2.9397 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5998 - mae: 0.5217 - val_loss: 15.2268 - val_mae: 2.9327 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5956 - mae: 0.5203 - val_loss: 15.1742 - val_mae: 2.9257 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5914 - mae: 0.5191 - val_loss: 15.1217 - val_mae: 2.9188 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5873 - mae: 0.5178 - val_loss: 15.0694 - val_mae: 2.9118 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5833 - mae: 0.5165 - val_loss: 15.0170 - val_mae: 2.9049 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5793 - mae: 0.5153 - val_loss: 14.9648 - val_mae: 2.8979 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5753 - mae: 0.5141 - val_loss: 14.9125 - val_mae: 2.8910 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5714 - mae: 0.5129 - val_loss: 14.8606 - val_mae: 2.8841 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5675 - mae: 0.5118 - val_loss: 14.8088 - val_mae: 2.8772 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5637 - mae: 0.5107 - val_loss: 14.7572 - val_mae: 2.8703 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5599 - mae: 0.5096 - val_loss: 14.7054 - val_mae: 2.8634 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5562 - mae: 0.5086 - val_loss: 14.6543 - val_mae: 2.8565 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5525 - mae: 0.5076 - val_loss: 14.6029 - val_mae: 2.8497 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5488 - mae: 0.5066 - val_loss: 14.5520 - val_mae: 2.8428 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5452 - mae: 0.5057 - val_loss: 14.5011 - val_mae: 2.8360 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5416 - mae: 0.5048 - val_loss: 14.4505 - val_mae: 2.8292 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5381 - mae: 0.5038 - val_loss: 14.4000 - val_mae: 2.8224 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5346 - mae: 0.5029 - val_loss: 14.3496 - val_mae: 2.8156 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5311 - mae: 0.5021 - val_loss: 14.2997 - val_mae: 2.8088 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5277 - mae: 0.5013 - val_loss: 14.2498 - val_mae: 2.8021 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5243 - mae: 0.5004 - val_loss: 14.2001 - val_mae: 2.7954 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5209 - mae: 0.4996 - val_loss: 14.1507 - val_mae: 2.7887 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5176 - mae: 0.4989 - val_loss: 14.1015 - val_mae: 2.7820 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5143 - mae: 0.4981 - val_loss: 14.0527 - val_mae: 2.7753 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5111 - mae: 0.4973 - val_loss: 14.0040 - val_mae: 2.7687 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5079 - mae: 0.4967 - val_loss: 13.9555 - val_mae: 2.7620 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5047 - mae: 0.4959 - val_loss: 13.9073 - val_mae: 2.7554 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.5015 - mae: 0.4953 - val_loss: 13.8594 - val_mae: 2.7489 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4984 - mae: 0.4946 - val_loss: 13.8118 - val_mae: 2.7423 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4953 - mae: 0.4940 - val_loss: 13.7645 - val_mae: 2.7358 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4923 - mae: 0.4933 - val_loss: 13.7173 - val_mae: 2.7293 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4893 - mae: 0.4927 - val_loss: 13.6706 - val_mae: 2.7228 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4863 - mae: 0.4921 - val_loss: 13.6242 - val_mae: 2.7163 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4833 - mae: 0.4915 - val_loss: 13.5779 - val_mae: 2.7099 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4804 - mae: 0.4909 - val_loss: 13.5319 - val_mae: 2.7035 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4775 - mae: 0.4904 - val_loss: 13.4863 - val_mae: 2.6971 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4747 - mae: 0.4898 - val_loss: 13.4409 - val_mae: 2.6908 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4718 - mae: 0.4893 - val_loss: 13.3959 - val_mae: 2.6844 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4690 - mae: 0.4887 - val_loss: 13.3513 - val_mae: 2.6782 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4663 - mae: 0.4883 - val_loss: 13.3069 - val_mae: 2.6719 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4635 - mae: 0.4877 - val_loss: 13.2630 - val_mae: 2.6657 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4608 - mae: 0.4873 - val_loss: 13.2192 - val_mae: 2.6595 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4581 - mae: 0.4868 - val_loss: 13.1757 - val_mae: 2.6533 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4555 - mae: 0.4863 - val_loss: 13.1327 - val_mae: 2.6472 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4529 - mae: 0.4858 - val_loss: 13.0898 - val_mae: 2.6411 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4503 - mae: 0.4854 - val_loss: 13.0476 - val_mae: 2.6350 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4477 - mae: 0.4850 - val_loss: 13.0054 - val_mae: 2.6290 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4452 - mae: 0.4846 - val_loss: 12.9636 - val_mae: 2.6230 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4427 - mae: 0.4842 - val_loss: 12.9223 - val_mae: 2.6171 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4402 - mae: 0.4838 - val_loss: 12.8812 - val_mae: 2.6111 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4377 - mae: 0.4834 - val_loss: 12.8405 - val_mae: 2.6052 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4353 - mae: 0.4830 - val_loss: 12.8000 - val_mae: 2.5993 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4329 - mae: 0.4826 - val_loss: 12.7599 - val_mae: 2.5935 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4305 - mae: 0.4822 - val_loss: 12.7204 - val_mae: 2.5877 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4282 - mae: 0.4818 - val_loss: 12.6810 - val_mae: 2.5820 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4259 - mae: 0.4815 - val_loss: 12.6421 - val_mae: 2.5763 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4235 - mae: 0.4812 - val_loss: 12.6034 - val_mae: 2.5706 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4213 - mae: 0.4809 - val_loss: 12.5651 - val_mae: 2.5650 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4190 - mae: 0.4805 - val_loss: 12.5272 - val_mae: 2.5594 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4168 - mae: 0.4802 - val_loss: 12.4897 - val_mae: 2.5538 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4146 - mae: 0.4799 - val_loss: 12.4525 - val_mae: 2.5483 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4125 - mae: 0.4796 - val_loss: 12.4156 - val_mae: 2.5428 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4103 - mae: 0.4792 - val_loss: 12.3790 - val_mae: 2.5373 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4082 - mae: 0.4790 - val_loss: 12.3430 - val_mae: 2.5319 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4061 - mae: 0.4787 - val_loss: 12.3070 - val_mae: 2.5265 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4040 - mae: 0.4784 - val_loss: 12.2715 - val_mae: 2.5212 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4020 - mae: 0.4782 - val_loss: 12.2364 - val_mae: 2.5159 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.4000 - mae: 0.4779 - val_loss: 12.2016 - val_mae: 2.5106 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3980 - mae: 0.4776 - val_loss: 12.1673 - val_mae: 2.5055 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3960 - mae: 0.4774 - val_loss: 12.1331 - val_mae: 2.5002 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3941 - mae: 0.4771 - val_loss: 12.0995 - val_mae: 2.4951 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3921 - mae: 0.4768 - val_loss: 12.0660 - val_mae: 2.4900 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3902 - mae: 0.4766 - val_loss: 12.0330 - val_mae: 2.4849 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3884 - mae: 0.4764 - val_loss: 12.0003 - val_mae: 2.4799 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3865 - mae: 0.4761 - val_loss: 11.9679 - val_mae: 2.4749 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3847 - mae: 0.4759 - val_loss: 11.9360 - val_mae: 2.4700 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3828 - mae: 0.4757 - val_loss: 11.9043 - val_mae: 2.4651 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3810 - mae: 0.4754 - val_loss: 11.8729 - val_mae: 2.4602 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3793 - mae: 0.4752 - val_loss: 11.8419 - val_mae: 2.4554 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3775 - mae: 0.4750 - val_loss: 11.8112 - val_mae: 2.4506 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3758 - mae: 0.4748 - val_loss: 11.7810 - val_mae: 2.4459 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3741 - mae: 0.4746 - val_loss: 11.7509 - val_mae: 2.4412 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3724 - mae: 0.4745 - val_loss: 11.7213 - val_mae: 2.4366 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3707 - mae: 0.4742 - val_loss: 11.6919 - val_mae: 2.4319 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3691 - mae: 0.4740 - val_loss: 11.6629 - val_mae: 2.4273 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3675 - mae: 0.4739 - val_loss: 11.6342 - val_mae: 2.4228 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3659 - mae: 0.4737 - val_loss: 11.6059 - val_mae: 2.4183 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3643 - mae: 0.4735 - val_loss: 11.5777 - val_mae: 2.4138 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3627 - mae: 0.4733 - val_loss: 11.5501 - val_mae: 2.4094 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3612 - mae: 0.4731 - val_loss: 11.5226 - val_mae: 2.4050 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3596 - mae: 0.4730 - val_loss: 11.4955 - val_mae: 2.4007 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3581 - mae: 0.4728 - val_loss: 11.4687 - val_mae: 2.3963 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3566 - mae: 0.4727 - val_loss: 11.4423 - val_mae: 2.3921 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3552 - mae: 0.4725 - val_loss: 11.4160 - val_mae: 2.3878 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3537 - mae: 0.4724 - val_loss: 11.3902 - val_mae: 2.3837 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3523 - mae: 0.4722 - val_loss: 11.3644 - val_mae: 2.3795 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3508 - mae: 0.4720 - val_loss: 11.3392 - val_mae: 2.3754 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3495 - mae: 0.4719 - val_loss: 11.3141 - val_mae: 2.3713 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3481 - mae: 0.4717 - val_loss: 11.2895 - val_mae: 2.3673 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3467 - mae: 0.4716 - val_loss: 11.2650 - val_mae: 2.3632 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3454 - mae: 0.4715 - val_loss: 11.2409 - val_mae: 2.3593 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3440 - mae: 0.4713 - val_loss: 11.2171 - val_mae: 2.3554 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3427 - mae: 0.4711 - val_loss: 11.1934 - val_mae: 2.3515 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3414 - mae: 0.4710 - val_loss: 11.1700 - val_mae: 2.3476 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3401 - mae: 0.4709 - val_loss: 11.1470 - val_mae: 2.3438 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3389 - mae: 0.4708 - val_loss: 11.1242 - val_mae: 2.3400 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3376 - mae: 0.4706 - val_loss: 11.1017 - val_mae: 2.3363 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3364 - mae: 0.4706 - val_loss: 11.0794 - val_mae: 2.3325 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3352 - mae: 0.4704 - val_loss: 11.0574 - val_mae: 2.3288 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3340 - mae: 0.4703 - val_loss: 11.0357 - val_mae: 2.3252 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3328 - mae: 0.4701 - val_loss: 11.0143 - val_mae: 2.3216 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3316 - mae: 0.4700 - val_loss: 10.9931 - val_mae: 2.3181 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3304 - mae: 0.4699 - val_loss: 10.9722 - val_mae: 2.3145 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3293 - mae: 0.4698 - val_loss: 10.9514 - val_mae: 2.3110 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3282 - mae: 0.4697 - val_loss: 10.9312 - val_mae: 2.3076 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3270 - mae: 0.4696 - val_loss: 10.9109 - val_mae: 2.3041 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3259 - mae: 0.4695 - val_loss: 10.8908 - val_mae: 2.3007 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3248 - mae: 0.4694 - val_loss: 10.8712 - val_mae: 2.2974 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3238 - mae: 0.4692 - val_loss: 10.8517 - val_mae: 2.2941 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3227 - mae: 0.4692 - val_loss: 10.8324 - val_mae: 2.2908 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3217 - mae: 0.4691 - val_loss: 10.8134 - val_mae: 2.2875 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3206 - mae: 0.4689 - val_loss: 10.7945 - val_mae: 2.2842 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3196 - mae: 0.4689 - val_loss: 10.7761 - val_mae: 2.2811 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3186 - mae: 0.4687 - val_loss: 10.7577 - val_mae: 2.2779 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3176 - mae: 0.4686 - val_loss: 10.7396 - val_mae: 2.2748 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3166 - mae: 0.4686 - val_loss: 10.7217 - val_mae: 2.2717 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3156 - mae: 0.4685 - val_loss: 10.7040 - val_mae: 2.2686 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3147 - mae: 0.4684 - val_loss: 10.6864 - val_mae: 2.2656 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3137 - mae: 0.4683 - val_loss: 10.6691 - val_mae: 2.2625 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3128 - mae: 0.4682 - val_loss: 10.6522 - val_mae: 2.2596 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3118 - mae: 0.4681 - val_loss: 10.6352 - val_mae: 2.2566 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3109 - mae: 0.4680 - val_loss: 10.6185 - val_mae: 2.2537 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3100 - mae: 0.4679 - val_loss: 10.6023 - val_mae: 2.2508 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3091 - mae: 0.4678 - val_loss: 10.5859 - val_mae: 2.2479 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3083 - mae: 0.4678 - val_loss: 10.5698 - val_mae: 2.2451 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3074 - mae: 0.4677 - val_loss: 10.5539 - val_mae: 2.2423 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3065 - mae: 0.4675 - val_loss: 10.5383 - val_mae: 2.2395 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3057 - mae: 0.4675 - val_loss: 10.5227 - val_mae: 2.2368 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3048 - mae: 0.4674 - val_loss: 10.5075 - val_mae: 2.2341 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3040 - mae: 0.4673 - val_loss: 10.4922 - val_mae: 2.2314 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3032 - mae: 0.4673 - val_loss: 10.4773 - val_mae: 2.2287 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3024 - mae: 0.4671 - val_loss: 10.4626 - val_mae: 2.2261 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3016 - mae: 0.4671 - val_loss: 10.4481 - val_mae: 2.2235 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3008 - mae: 0.4670 - val_loss: 10.4336 - val_mae: 2.2209 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.3000 - mae: 0.4669 - val_loss: 10.4194 - val_mae: 2.2184 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2992 - mae: 0.4668 - val_loss: 10.4053 - val_mae: 2.2158 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2985 - mae: 0.4668 - val_loss: 10.3914 - val_mae: 2.2133 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2977 - mae: 0.4667 - val_loss: 10.3777 - val_mae: 2.2108 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2970 - mae: 0.4667 - val_loss: 10.3642 - val_mae: 2.2084 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2962 - mae: 0.4666 - val_loss: 10.3506 - val_mae: 2.2059 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2955 - mae: 0.4665 - val_loss: 10.3375 - val_mae: 2.2036 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2948 - mae: 0.4664 - val_loss: 10.3243 - val_mae: 2.2012 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2941 - mae: 0.4664 - val_loss: 10.3115 - val_mae: 2.1988 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2934 - mae: 0.4663 - val_loss: 10.2985 - val_mae: 2.1965 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2927 - mae: 0.4662 - val_loss: 10.2859 - val_mae: 2.1942 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2920 - mae: 0.4662 - val_loss: 10.2733 - val_mae: 2.1919 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2913 - mae: 0.4661 - val_loss: 10.2610 - val_mae: 2.1896 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2906 - mae: 0.4660 - val_loss: 10.2488 - val_mae: 2.1874 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2900 - mae: 0.4660 - val_loss: 10.2367 - val_mae: 2.1852 - lr: 0.0010\n",
            "Epoch 297/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2893 - mae: 0.4658 - val_loss: 10.2249 - val_mae: 2.1830 - lr: 0.0010\n",
            "Epoch 298/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2887 - mae: 0.4658 - val_loss: 10.2131 - val_mae: 2.1808 - lr: 0.0010\n",
            "Epoch 299/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2880 - mae: 0.4657 - val_loss: 10.2014 - val_mae: 2.1787 - lr: 0.0010\n",
            "Epoch 300/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2874 - mae: 0.4657 - val_loss: 10.1899 - val_mae: 2.1766 - lr: 0.0010\n",
            "Epoch 301/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2868 - mae: 0.4656 - val_loss: 10.1784 - val_mae: 2.1745 - lr: 0.0010\n",
            "Epoch 302/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2862 - mae: 0.4655 - val_loss: 10.1672 - val_mae: 2.1724 - lr: 0.0010\n",
            "Epoch 303/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2856 - mae: 0.4655 - val_loss: 10.1561 - val_mae: 2.1703 - lr: 0.0010\n",
            "Epoch 304/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2849 - mae: 0.4654 - val_loss: 10.1450 - val_mae: 2.1683 - lr: 0.0010\n",
            "Epoch 305/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2844 - mae: 0.4654 - val_loss: 10.1342 - val_mae: 2.1663 - lr: 0.0010\n",
            "Epoch 306/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2838 - mae: 0.4653 - val_loss: 10.1234 - val_mae: 2.1642 - lr: 0.0010\n",
            "Epoch 307/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2832 - mae: 0.4652 - val_loss: 10.1129 - val_mae: 2.1623 - lr: 0.0010\n",
            "Epoch 308/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2826 - mae: 0.4651 - val_loss: 10.1024 - val_mae: 2.1603 - lr: 0.0010\n",
            "Epoch 309/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2820 - mae: 0.4651 - val_loss: 10.0920 - val_mae: 2.1584 - lr: 0.0010\n",
            "Epoch 310/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2815 - mae: 0.4650 - val_loss: 10.0818 - val_mae: 2.1565 - lr: 0.0010\n",
            "Epoch 311/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2809 - mae: 0.4650 - val_loss: 10.0716 - val_mae: 2.1546 - lr: 0.0010\n",
            "Epoch 312/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2804 - mae: 0.4649 - val_loss: 10.0615 - val_mae: 2.1527 - lr: 0.0010\n",
            "Epoch 313/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2798 - mae: 0.4649 - val_loss: 10.0517 - val_mae: 2.1508 - lr: 0.0010\n",
            "Epoch 314/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2793 - mae: 0.4648 - val_loss: 10.0419 - val_mae: 2.1490 - lr: 0.0010\n",
            "Epoch 315/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2787 - mae: 0.4647 - val_loss: 10.0321 - val_mae: 2.1472 - lr: 0.0010\n",
            "Epoch 316/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2782 - mae: 0.4647 - val_loss: 10.0226 - val_mae: 2.1454 - lr: 0.0010\n",
            "Epoch 317/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2777 - mae: 0.4646 - val_loss: 10.0132 - val_mae: 2.1436 - lr: 0.0010\n",
            "Epoch 318/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2772 - mae: 0.4645 - val_loss: 10.0037 - val_mae: 2.1418 - lr: 0.0010\n",
            "Epoch 319/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2767 - mae: 0.4645 - val_loss: 9.9946 - val_mae: 2.1401 - lr: 0.0010\n",
            "Epoch 320/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2762 - mae: 0.4644 - val_loss: 9.9854 - val_mae: 2.1383 - lr: 0.0010\n",
            "Epoch 321/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2757 - mae: 0.4644 - val_loss: 9.9764 - val_mae: 2.1366 - lr: 0.0010\n",
            "Epoch 322/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2752 - mae: 0.4643 - val_loss: 9.9673 - val_mae: 2.1349 - lr: 0.0010\n",
            "Epoch 323/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2747 - mae: 0.4643 - val_loss: 9.9585 - val_mae: 2.1332 - lr: 0.0010\n",
            "Epoch 324/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2742 - mae: 0.4642 - val_loss: 9.9498 - val_mae: 2.1315 - lr: 0.0010\n",
            "Epoch 325/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2737 - mae: 0.4641 - val_loss: 9.9412 - val_mae: 2.1299 - lr: 0.0010\n",
            "Epoch 326/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2732 - mae: 0.4641 - val_loss: 9.9325 - val_mae: 2.1283 - lr: 0.0010\n",
            "Epoch 327/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2728 - mae: 0.4640 - val_loss: 9.9241 - val_mae: 2.1267 - lr: 0.0010\n",
            "Epoch 328/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2723 - mae: 0.4640 - val_loss: 9.9158 - val_mae: 2.1251 - lr: 0.0010\n",
            "Epoch 329/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2718 - mae: 0.4639 - val_loss: 9.9075 - val_mae: 2.1235 - lr: 0.0010\n",
            "Epoch 330/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2714 - mae: 0.4639 - val_loss: 9.8993 - val_mae: 2.1219 - lr: 0.0010\n",
            "Epoch 331/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2709 - mae: 0.4638 - val_loss: 9.8912 - val_mae: 2.1204 - lr: 0.0010\n",
            "Epoch 332/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2705 - mae: 0.4637 - val_loss: 9.8832 - val_mae: 2.1188 - lr: 0.0010\n",
            "Epoch 333/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2701 - mae: 0.4637 - val_loss: 9.8754 - val_mae: 2.1174 - lr: 0.0010\n",
            "Epoch 334/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2696 - mae: 0.4636 - val_loss: 9.8674 - val_mae: 2.1158 - lr: 0.0010\n",
            "Epoch 335/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2692 - mae: 0.4636 - val_loss: 9.8597 - val_mae: 2.1144 - lr: 0.0010\n",
            "Epoch 336/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2688 - mae: 0.4635 - val_loss: 9.8521 - val_mae: 2.1129 - lr: 0.0010\n",
            "Epoch 337/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2683 - mae: 0.4635 - val_loss: 9.8445 - val_mae: 2.1114 - lr: 0.0010\n",
            "Epoch 338/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2679 - mae: 0.4634 - val_loss: 9.8369 - val_mae: 2.1099 - lr: 0.0010\n",
            "Epoch 339/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2675 - mae: 0.4634 - val_loss: 9.8295 - val_mae: 2.1085 - lr: 0.0010\n",
            "Epoch 340/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2671 - mae: 0.4633 - val_loss: 9.8222 - val_mae: 2.1071 - lr: 0.0010\n",
            "Epoch 341/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2667 - mae: 0.4633 - val_loss: 9.8149 - val_mae: 2.1057 - lr: 0.0010\n",
            "Epoch 342/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2663 - mae: 0.4632 - val_loss: 9.8078 - val_mae: 2.1043 - lr: 0.0010\n",
            "Epoch 343/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2659 - mae: 0.4631 - val_loss: 9.8007 - val_mae: 2.1029 - lr: 0.0010\n",
            "Epoch 344/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2655 - mae: 0.4631 - val_loss: 9.7936 - val_mae: 2.1015 - lr: 0.0010\n",
            "Epoch 345/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2651 - mae: 0.4630 - val_loss: 9.7866 - val_mae: 2.1002 - lr: 0.0010\n",
            "Epoch 346/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2647 - mae: 0.4630 - val_loss: 9.7798 - val_mae: 2.0988 - lr: 0.0010\n",
            "Epoch 347/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2643 - mae: 0.4630 - val_loss: 9.7729 - val_mae: 2.0975 - lr: 0.0010\n",
            "Epoch 348/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2640 - mae: 0.4629 - val_loss: 9.7662 - val_mae: 2.0962 - lr: 0.0010\n",
            "Epoch 349/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2636 - mae: 0.4629 - val_loss: 9.7594 - val_mae: 2.0949 - lr: 0.0010\n",
            "Epoch 350/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2632 - mae: 0.4628 - val_loss: 9.7529 - val_mae: 2.0936 - lr: 0.0010\n",
            "Epoch 351/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2628 - mae: 0.4628 - val_loss: 9.7464 - val_mae: 2.0923 - lr: 0.0010\n",
            "Epoch 352/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2625 - mae: 0.4626 - val_loss: 9.7399 - val_mae: 2.0911 - lr: 0.0010\n",
            "Epoch 353/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2621 - mae: 0.4626 - val_loss: 9.7335 - val_mae: 2.0898 - lr: 0.0010\n",
            "Epoch 354/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2618 - mae: 0.4626 - val_loss: 9.7272 - val_mae: 2.0886 - lr: 0.0010\n",
            "Epoch 355/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2614 - mae: 0.4625 - val_loss: 9.7209 - val_mae: 2.0873 - lr: 0.0010\n",
            "Epoch 356/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2611 - mae: 0.4625 - val_loss: 9.7147 - val_mae: 2.0861 - lr: 0.0010\n",
            "Epoch 357/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2607 - mae: 0.4624 - val_loss: 9.7086 - val_mae: 2.0849 - lr: 0.0010\n",
            "Epoch 358/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2604 - mae: 0.4624 - val_loss: 9.7026 - val_mae: 2.0837 - lr: 0.0010\n",
            "Epoch 359/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2600 - mae: 0.4623 - val_loss: 9.6966 - val_mae: 2.0826 - lr: 0.0010\n",
            "Epoch 360/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2597 - mae: 0.4623 - val_loss: 9.6906 - val_mae: 2.0813 - lr: 0.0010\n",
            "Epoch 361/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2593 - mae: 0.4622 - val_loss: 9.6847 - val_mae: 2.0802 - lr: 0.0010\n",
            "Epoch 362/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2590 - mae: 0.4621 - val_loss: 9.6790 - val_mae: 2.0791 - lr: 0.0010\n",
            "Epoch 363/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2587 - mae: 0.4621 - val_loss: 9.6732 - val_mae: 2.0779 - lr: 0.0010\n",
            "Epoch 364/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2584 - mae: 0.4621 - val_loss: 9.6674 - val_mae: 2.0768 - lr: 0.0010\n",
            "Epoch 365/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2580 - mae: 0.4620 - val_loss: 9.6618 - val_mae: 2.0757 - lr: 0.0010\n",
            "Epoch 366/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2577 - mae: 0.4620 - val_loss: 9.6562 - val_mae: 2.0746 - lr: 0.0010\n",
            "Epoch 367/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2574 - mae: 0.4619 - val_loss: 9.6506 - val_mae: 2.0735 - lr: 0.0010\n",
            "Epoch 368/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2571 - mae: 0.4619 - val_loss: 9.6451 - val_mae: 2.0724 - lr: 0.0010\n",
            "Epoch 369/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2568 - mae: 0.4618 - val_loss: 9.6397 - val_mae: 2.0713 - lr: 0.0010\n",
            "Epoch 370/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2565 - mae: 0.4618 - val_loss: 9.6344 - val_mae: 2.0702 - lr: 0.0010\n",
            "Epoch 371/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2562 - mae: 0.4617 - val_loss: 9.6291 - val_mae: 2.0692 - lr: 0.0010\n",
            "Epoch 372/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2559 - mae: 0.4617 - val_loss: 9.6238 - val_mae: 2.0682 - lr: 0.0010\n",
            "Epoch 373/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2556 - mae: 0.4616 - val_loss: 9.6186 - val_mae: 2.0671 - lr: 0.0010\n",
            "Epoch 374/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2553 - mae: 0.4616 - val_loss: 9.6134 - val_mae: 2.0661 - lr: 0.0010\n",
            "Epoch 375/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2550 - mae: 0.4615 - val_loss: 9.6083 - val_mae: 2.0651 - lr: 0.0010\n",
            "Epoch 376/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2547 - mae: 0.4615 - val_loss: 9.6033 - val_mae: 2.0640 - lr: 0.0010\n",
            "Epoch 377/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2544 - mae: 0.4614 - val_loss: 9.5982 - val_mae: 2.0630 - lr: 0.0010\n",
            "Epoch 378/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2541 - mae: 0.4614 - val_loss: 9.5932 - val_mae: 2.0620 - lr: 0.0010\n",
            "Epoch 379/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2538 - mae: 0.4613 - val_loss: 9.5883 - val_mae: 2.0610 - lr: 0.0010\n",
            "Epoch 380/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2535 - mae: 0.4613 - val_loss: 9.5835 - val_mae: 2.0601 - lr: 0.0010\n",
            "Epoch 381/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2533 - mae: 0.4612 - val_loss: 9.5787 - val_mae: 2.0591 - lr: 0.0010\n",
            "Epoch 382/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2530 - mae: 0.4612 - val_loss: 9.5739 - val_mae: 2.0582 - lr: 0.0010\n",
            "Epoch 383/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2527 - mae: 0.4612 - val_loss: 9.5692 - val_mae: 2.0572 - lr: 0.0010\n",
            "Epoch 384/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2525 - mae: 0.4611 - val_loss: 9.5646 - val_mae: 2.0563 - lr: 0.0010\n",
            "Epoch 385/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2522 - mae: 0.4610 - val_loss: 9.5598 - val_mae: 2.0553 - lr: 0.0010\n",
            "Epoch 386/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2519 - mae: 0.4610 - val_loss: 9.5553 - val_mae: 2.0544 - lr: 0.0010\n",
            "Epoch 387/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2516 - mae: 0.4609 - val_loss: 9.5506 - val_mae: 2.0535 - lr: 0.0010\n",
            "Epoch 388/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2514 - mae: 0.4609 - val_loss: 9.5461 - val_mae: 2.0526 - lr: 0.0010\n",
            "Epoch 389/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2511 - mae: 0.4609 - val_loss: 9.5418 - val_mae: 2.0517 - lr: 0.0010\n",
            "Epoch 390/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2509 - mae: 0.4608 - val_loss: 9.5372 - val_mae: 2.0508 - lr: 0.0010\n",
            "Epoch 391/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2506 - mae: 0.4607 - val_loss: 9.5329 - val_mae: 2.0499 - lr: 0.0010\n",
            "Epoch 392/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2503 - mae: 0.4607 - val_loss: 9.5286 - val_mae: 2.0491 - lr: 0.0010\n",
            "Epoch 393/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2501 - mae: 0.4607 - val_loss: 9.5243 - val_mae: 2.0482 - lr: 0.0010\n",
            "Epoch 394/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2498 - mae: 0.4606 - val_loss: 9.5200 - val_mae: 2.0473 - lr: 0.0010\n",
            "Epoch 395/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2496 - mae: 0.4606 - val_loss: 9.5158 - val_mae: 2.0465 - lr: 0.0010\n",
            "Epoch 396/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2493 - mae: 0.4605 - val_loss: 9.5117 - val_mae: 2.0456 - lr: 0.0010\n",
            "Epoch 397/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2491 - mae: 0.4605 - val_loss: 9.5074 - val_mae: 2.0448 - lr: 0.0010\n",
            "Epoch 398/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2489 - mae: 0.4604 - val_loss: 9.5033 - val_mae: 2.0439 - lr: 0.0010\n",
            "Epoch 399/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2486 - mae: 0.4604 - val_loss: 9.4992 - val_mae: 2.0431 - lr: 0.0010\n",
            "Epoch 400/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2484 - mae: 0.4604 - val_loss: 9.4953 - val_mae: 2.0423 - lr: 0.0010\n",
            "Epoch 401/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2481 - mae: 0.4603 - val_loss: 9.4912 - val_mae: 2.0415 - lr: 0.0010\n",
            "Epoch 402/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2479 - mae: 0.4603 - val_loss: 9.4873 - val_mae: 2.0407 - lr: 0.0010\n",
            "Epoch 403/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2477 - mae: 0.4602 - val_loss: 9.4834 - val_mae: 2.0399 - lr: 0.0010\n",
            "Epoch 404/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2475 - mae: 0.4602 - val_loss: 9.4796 - val_mae: 2.0391 - lr: 0.0010\n",
            "Epoch 405/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2472 - mae: 0.4601 - val_loss: 9.4757 - val_mae: 2.0383 - lr: 0.0010\n",
            "Epoch 406/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2470 - mae: 0.4601 - val_loss: 9.4718 - val_mae: 2.0375 - lr: 0.0010\n",
            "Epoch 407/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2468 - mae: 0.4600 - val_loss: 9.4681 - val_mae: 2.0368 - lr: 0.0010\n",
            "Epoch 408/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2466 - mae: 0.4600 - val_loss: 9.4643 - val_mae: 2.0360 - lr: 0.0010\n",
            "Epoch 409/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2463 - mae: 0.4599 - val_loss: 9.4606 - val_mae: 2.0353 - lr: 0.0010\n",
            "Epoch 410/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2461 - mae: 0.4599 - val_loss: 9.4569 - val_mae: 2.0345 - lr: 0.0010\n",
            "Epoch 411/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2459 - mae: 0.4599 - val_loss: 9.4533 - val_mae: 2.0337 - lr: 0.0010\n",
            "Epoch 412/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2457 - mae: 0.4598 - val_loss: 9.4495 - val_mae: 2.0330 - lr: 0.0010\n",
            "Epoch 413/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2455 - mae: 0.4598 - val_loss: 9.4461 - val_mae: 2.0323 - lr: 0.0010\n",
            "Epoch 414/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2452 - mae: 0.4597 - val_loss: 9.4426 - val_mae: 2.0315 - lr: 0.0010\n",
            "Epoch 415/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2450 - mae: 0.4597 - val_loss: 9.4389 - val_mae: 2.0308 - lr: 0.0010\n",
            "Epoch 416/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2448 - mae: 0.4596 - val_loss: 9.4355 - val_mae: 2.0301 - lr: 0.0010\n",
            "Epoch 417/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2446 - mae: 0.4596 - val_loss: 9.4321 - val_mae: 2.0294 - lr: 0.0010\n",
            "Epoch 418/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2444 - mae: 0.4595 - val_loss: 9.4287 - val_mae: 2.0288 - lr: 0.0010\n",
            "Epoch 419/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2442 - mae: 0.4595 - val_loss: 9.4252 - val_mae: 2.0280 - lr: 0.0010\n",
            "Epoch 420/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2440 - mae: 0.4595 - val_loss: 9.4218 - val_mae: 2.0273 - lr: 0.0010\n",
            "Epoch 421/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2438 - mae: 0.4594 - val_loss: 9.4185 - val_mae: 2.0266 - lr: 0.0010\n",
            "Epoch 422/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2436 - mae: 0.4594 - val_loss: 9.4152 - val_mae: 2.0259 - lr: 0.0010\n",
            "Epoch 423/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2434 - mae: 0.4593 - val_loss: 9.4119 - val_mae: 2.0253 - lr: 0.0010\n",
            "Epoch 424/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2432 - mae: 0.4593 - val_loss: 9.4086 - val_mae: 2.0246 - lr: 0.0010\n",
            "Epoch 425/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2430 - mae: 0.4592 - val_loss: 9.4054 - val_mae: 2.0239 - lr: 0.0010\n",
            "Epoch 426/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2428 - mae: 0.4592 - val_loss: 9.4022 - val_mae: 2.0233 - lr: 0.0010\n",
            "Epoch 427/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2426 - mae: 0.4592 - val_loss: 9.3991 - val_mae: 2.0227 - lr: 0.0010\n",
            "Epoch 428/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2424 - mae: 0.4591 - val_loss: 9.3959 - val_mae: 2.0220 - lr: 0.0010\n",
            "Epoch 429/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2422 - mae: 0.4591 - val_loss: 9.3928 - val_mae: 2.0214 - lr: 0.0010\n",
            "Epoch 430/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2420 - mae: 0.4590 - val_loss: 9.3897 - val_mae: 2.0207 - lr: 0.0010\n",
            "Epoch 431/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2419 - mae: 0.4590 - val_loss: 9.3866 - val_mae: 2.0201 - lr: 0.0010\n",
            "Epoch 432/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2417 - mae: 0.4589 - val_loss: 9.3836 - val_mae: 2.0195 - lr: 0.0010\n",
            "Epoch 433/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2415 - mae: 0.4589 - val_loss: 9.3806 - val_mae: 2.0188 - lr: 0.0010\n",
            "Epoch 434/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2413 - mae: 0.4589 - val_loss: 9.3776 - val_mae: 2.0182 - lr: 0.0010\n",
            "Epoch 435/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2411 - mae: 0.4588 - val_loss: 9.3746 - val_mae: 2.0176 - lr: 0.0010\n",
            "Epoch 436/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2409 - mae: 0.4588 - val_loss: 9.3717 - val_mae: 2.0170 - lr: 0.0010\n",
            "Epoch 437/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2407 - mae: 0.4587 - val_loss: 9.3688 - val_mae: 2.0164 - lr: 0.0010\n",
            "Epoch 438/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2406 - mae: 0.4587 - val_loss: 9.3658 - val_mae: 2.0158 - lr: 0.0010\n",
            "Epoch 439/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2404 - mae: 0.4586 - val_loss: 9.3629 - val_mae: 2.0152 - lr: 0.0010\n",
            "Epoch 440/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2402 - mae: 0.4586 - val_loss: 9.3602 - val_mae: 2.0146 - lr: 0.0010\n",
            "Epoch 441/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2400 - mae: 0.4585 - val_loss: 9.3573 - val_mae: 2.0140 - lr: 0.0010\n",
            "Epoch 442/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2399 - mae: 0.4585 - val_loss: 9.3546 - val_mae: 2.0134 - lr: 0.0010\n",
            "Epoch 443/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2397 - mae: 0.4585 - val_loss: 9.3516 - val_mae: 2.0128 - lr: 0.0010\n",
            "Epoch 444/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2395 - mae: 0.4585 - val_loss: 9.3490 - val_mae: 2.0123 - lr: 0.0010\n",
            "Epoch 445/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2393 - mae: 0.4584 - val_loss: 9.3462 - val_mae: 2.0117 - lr: 0.0010\n",
            "Epoch 446/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2392 - mae: 0.4583 - val_loss: 9.3435 - val_mae: 2.0112 - lr: 0.0010\n",
            "Epoch 447/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2390 - mae: 0.4583 - val_loss: 9.3408 - val_mae: 2.0106 - lr: 0.0010\n",
            "Epoch 448/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2388 - mae: 0.4582 - val_loss: 9.3382 - val_mae: 2.0101 - lr: 0.0010\n",
            "Epoch 449/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2387 - mae: 0.4583 - val_loss: 9.3355 - val_mae: 2.0095 - lr: 0.0010\n",
            "Epoch 450/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2385 - mae: 0.4582 - val_loss: 9.3329 - val_mae: 2.0090 - lr: 0.0010\n",
            "Epoch 451/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2383 - mae: 0.4582 - val_loss: 9.3304 - val_mae: 2.0085 - lr: 0.0010\n",
            "Epoch 452/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2382 - mae: 0.4581 - val_loss: 9.3278 - val_mae: 2.0079 - lr: 0.0010\n",
            "Epoch 453/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2380 - mae: 0.4581 - val_loss: 9.3251 - val_mae: 2.0074 - lr: 0.0010\n",
            "Epoch 454/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2379 - mae: 0.4580 - val_loss: 9.3227 - val_mae: 2.0068 - lr: 0.0010\n",
            "Epoch 455/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2377 - mae: 0.4580 - val_loss: 9.3201 - val_mae: 2.0063 - lr: 0.0010\n",
            "Epoch 456/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2375 - mae: 0.4580 - val_loss: 9.3177 - val_mae: 2.0058 - lr: 0.0010\n",
            "Epoch 457/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2374 - mae: 0.4579 - val_loss: 9.3152 - val_mae: 2.0053 - lr: 0.0010\n",
            "Epoch 458/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2372 - mae: 0.4578 - val_loss: 9.3127 - val_mae: 2.0048 - lr: 0.0010\n",
            "Epoch 459/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2371 - mae: 0.4578 - val_loss: 9.3102 - val_mae: 2.0042 - lr: 0.0010\n",
            "Epoch 460/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2369 - mae: 0.4578 - val_loss: 9.3078 - val_mae: 2.0037 - lr: 0.0010\n",
            "Epoch 461/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2368 - mae: 0.4577 - val_loss: 9.3053 - val_mae: 2.0032 - lr: 0.0010\n",
            "Epoch 462/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2366 - mae: 0.4577 - val_loss: 9.3031 - val_mae: 2.0027 - lr: 0.0010\n",
            "Epoch 463/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2365 - mae: 0.4577 - val_loss: 9.3006 - val_mae: 2.0022 - lr: 0.0010\n",
            "Epoch 464/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2363 - mae: 0.4577 - val_loss: 9.2983 - val_mae: 2.0017 - lr: 0.0010\n",
            "Epoch 465/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2362 - mae: 0.4576 - val_loss: 9.2959 - val_mae: 2.0013 - lr: 0.0010\n",
            "Epoch 466/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2360 - mae: 0.4576 - val_loss: 9.2936 - val_mae: 2.0008 - lr: 0.0010\n",
            "Epoch 467/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2359 - mae: 0.4575 - val_loss: 9.2912 - val_mae: 2.0003 - lr: 0.0010\n",
            "Epoch 468/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2357 - mae: 0.4575 - val_loss: 9.2889 - val_mae: 1.9998 - lr: 0.0010\n",
            "Epoch 469/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2356 - mae: 0.4574 - val_loss: 9.2868 - val_mae: 1.9993 - lr: 0.0010\n",
            "Epoch 470/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2354 - mae: 0.4574 - val_loss: 9.2845 - val_mae: 1.9988 - lr: 0.0010\n",
            "Epoch 471/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2353 - mae: 0.4573 - val_loss: 9.2822 - val_mae: 1.9984 - lr: 0.0010\n",
            "Epoch 472/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2352 - mae: 0.4573 - val_loss: 9.2800 - val_mae: 1.9979 - lr: 0.0010\n",
            "Epoch 473/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2350 - mae: 0.4572 - val_loss: 9.2778 - val_mae: 1.9974 - lr: 0.0010\n",
            "Epoch 474/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2349 - mae: 0.4572 - val_loss: 9.2755 - val_mae: 1.9970 - lr: 0.0010\n",
            "Epoch 475/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2347 - mae: 0.4572 - val_loss: 9.2734 - val_mae: 1.9965 - lr: 0.0010\n",
            "Epoch 476/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2346 - mae: 0.4572 - val_loss: 9.2712 - val_mae: 1.9961 - lr: 0.0010\n",
            "Epoch 477/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2344 - mae: 0.4571 - val_loss: 9.2691 - val_mae: 1.9956 - lr: 0.0010\n",
            "Epoch 478/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2343 - mae: 0.4571 - val_loss: 9.2670 - val_mae: 1.9952 - lr: 0.0010\n",
            "Epoch 479/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2342 - mae: 0.4570 - val_loss: 9.2650 - val_mae: 1.9947 - lr: 0.0010\n",
            "Epoch 480/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2340 - mae: 0.4570 - val_loss: 9.2628 - val_mae: 1.9943 - lr: 0.0010\n",
            "Epoch 481/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2339 - mae: 0.4570 - val_loss: 9.2608 - val_mae: 1.9939 - lr: 0.0010\n",
            "Epoch 482/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2338 - mae: 0.4569 - val_loss: 9.2586 - val_mae: 1.9935 - lr: 0.0010\n",
            "Epoch 483/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2336 - mae: 0.4569 - val_loss: 9.2567 - val_mae: 1.9930 - lr: 0.0010\n",
            "Epoch 484/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2335 - mae: 0.4568 - val_loss: 9.2547 - val_mae: 1.9926 - lr: 0.0010\n",
            "Epoch 485/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2334 - mae: 0.4568 - val_loss: 9.2527 - val_mae: 1.9922 - lr: 0.0010\n",
            "Epoch 486/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2332 - mae: 0.4568 - val_loss: 9.2506 - val_mae: 1.9917 - lr: 0.0010\n",
            "Epoch 487/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2331 - mae: 0.4568 - val_loss: 9.2487 - val_mae: 1.9913 - lr: 0.0010\n",
            "Epoch 488/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2330 - mae: 0.4567 - val_loss: 9.2466 - val_mae: 1.9909 - lr: 0.0010\n",
            "Epoch 489/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2328 - mae: 0.4567 - val_loss: 9.2447 - val_mae: 1.9905 - lr: 0.0010\n",
            "Epoch 490/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2327 - mae: 0.4566 - val_loss: 9.2428 - val_mae: 1.9901 - lr: 0.0010\n",
            "Epoch 491/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2326 - mae: 0.4566 - val_loss: 9.2409 - val_mae: 1.9897 - lr: 0.0010\n",
            "Epoch 492/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2325 - mae: 0.4566 - val_loss: 9.2389 - val_mae: 1.9893 - lr: 0.0010\n",
            "Epoch 493/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2323 - mae: 0.4565 - val_loss: 9.2372 - val_mae: 1.9889 - lr: 0.0010\n",
            "Epoch 494/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2322 - mae: 0.4565 - val_loss: 9.2351 - val_mae: 1.9885 - lr: 0.0010\n",
            "Epoch 495/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2321 - mae: 0.4565 - val_loss: 9.2332 - val_mae: 1.9881 - lr: 0.0010\n",
            "Epoch 496/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2320 - mae: 0.4564 - val_loss: 9.2314 - val_mae: 1.9877 - lr: 0.0010\n",
            "Epoch 497/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2318 - mae: 0.4564 - val_loss: 9.2295 - val_mae: 1.9873 - lr: 0.0010\n",
            "Epoch 498/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2317 - mae: 0.4563 - val_loss: 9.2277 - val_mae: 1.9869 - lr: 0.0010\n",
            "Epoch 499/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2316 - mae: 0.4563 - val_loss: 9.2260 - val_mae: 1.9865 - lr: 0.0010\n",
            "Epoch 500/500\n",
            "82/82 [==============================] - 1s 10ms/step - loss: 3.2315 - mae: 0.4563 - val_loss: 9.2241 - val_mae: 1.9862 - lr: 0.0010\n",
            "43518/43518 [==============================] - 59s 1ms/step\n",
            "CPU times: user 13min 33s, sys: 48.5 s, total: 14min 21s\n",
            "Wall time: 8min 15s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Entrenar el modelo con el conjunto de entrenamiento\n",
        "model.fit([user_ids_train, item_ids_train], ratings_train, validation_data=([user_ids_test, item_ids_test], ratings_test), epochs=500, batch_size=40000, verbose=1, callbacks=[early_stop, reduce_lr])\n",
        "\n",
        "predictions = model.predict([user_ids_test, item_ids_test])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMJd_nt7g5h4",
        "outputId": "d21e2574-e41d-4778-d1aa-2c79da6d4661"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35/35 [==============================] - 0s 3ms/step - loss: 9.2241 - mae: 1.9862\n",
            "Test Loss: 9.2241\n",
            "Test MAE: 1.9862\n"
          ]
        }
      ],
      "source": [
        "# Evaluar el modelo con el conjunto de prueba\n",
        "loss, mae = model.evaluate([user_ids_test, item_ids_test], ratings_test, batch_size=40000)\n",
        "print(f'Test Loss: {loss:.4f}')\n",
        "print(f'Test MAE: {mae:.4f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USnKRkm0KCdW",
        "outputId": "3a57b940-e7ac-4c76-cdd4-e1c74fda6647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 6.4101\n",
            "RMSE: 2.5318\n",
            "MAE: 1.9862\n",
            "MAPE: 50.87%\n"
          ]
        }
      ],
      "source": [
        "# MSE\n",
        "mse = mean_squared_error(ratings_test, predictions)\n",
        "print(f\"MSE: {mse:.4f}\")\n",
        "\n",
        "# RMSE\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"RMSE: {rmse:.4f}\")\n",
        "\n",
        "# MAE\n",
        "mae = mean_absolute_error(ratings_test, predictions)\n",
        "print(f\"MAE: {mae:.4f}\")\n",
        "\n",
        "# MAPE (Mean Absolute Percentage Error)\n",
        "mape = np.mean(np.abs((ratings_test - predictions.flatten()) / ratings_test)) * 100\n",
        "print(f\"MAPE: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odgbIWArkJ2y"
      },
      "source": [
        "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa con Keras:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBK_zPK7lvti"
      },
      "source": [
        "Funcionamiento:\n",
        "\n",
        "Estos sistemas toman características de los ítems y/o usuarios y las pasan a través de una o varias capas de neuronas para obtener una predicción.\n",
        "Las redes neuronales son capaces de capturar interacciones no lineales entre características, lo que las hace poderosas para tareas de modelado complejas.\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "Capacidad de modelar relaciones no lineales.\n",
        "\n",
        "Flexibilidad para incorporar múltiples fuentes de datos o características.\n",
        "\n",
        "Puede manejar arranques en frío al incorporar características de nuevos ítems o usuarios.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "Mayor costo computacional en comparación con los sistemas basados únicamente en embeddings.\n",
        "\n",
        "Riesgo de sobreajuste si no se tiene un conjunto de datos lo suficientemente grande."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMnns-ENh_KJ"
      },
      "source": [
        "Ahora el código incluye regularización en los embeddings, early stopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validación, y reducción de la tasa de aprendizaje si el error en el conjunto de validación no mejora después de algunas épocas. Estas adiciones deberían ayudar a mejorar la capacidad de generalización del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ZFgVZSeTkKGy"
      },
      "outputs": [],
      "source": [
        "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
        "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
        "\n",
        "# Número de usuarios e ítems\n",
        "n_users = df['user_id'].nunique()\n",
        "n_items = df['item_id'].nunique()\n",
        "\n",
        "# Hiperparámetros\n",
        "hidden_units = [128, 64, 32]\n",
        "dropout_rate = 0.2\n",
        "l2_reg = 1e-6\n",
        "\n",
        "# Arquitectura del modelo\n",
        "user_input = Input(shape=[1], name='user_input')\n",
        "item_input = Input(shape=[1], name='item_input')\n",
        "concat = Concatenate()([user_input, item_input])\n",
        "dense = concat\n",
        "for units in hidden_units:\n",
        "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
        "    dense = Dropout(dropout_rate)(dense)\n",
        "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)  # También puedes aplicar regularización L2 aquí\n",
        "model = Model(inputs=[user_input, item_input], outputs=output)\n",
        "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "N7uvl8VEhqdD"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "blU4Q9akkVBc"
      },
      "outputs": [],
      "source": [
        "# Datos para entrenamiento\n",
        "user_data = df['user_id'].values\n",
        "item_data = df['item_id'].values\n",
        "rating_data = df['overall'].values\n",
        "\n",
        "# Dividir los datos\n",
        "user_data_train, user_data_val, item_data_train, item_data_val, ratings_train, ratings_val = train_test_split(\n",
        "    user_data, item_data, rating_data, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Entrenamiento del modelo\n",
        "history = model.fit([user_data_train, item_data_train], ratings_train,\n",
        "    epochs=500,validation_data=([user_data_val, item_data_val], ratings_val),\n",
        "    batch_size=40000,verbose=1,callbacks=[early_stop, reduce_lr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGxLwUhzf32z",
        "outputId": "67dcde39-2f67-4293-e5a5-6d3e9449a945"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "82/82 [==============================] - 3s 10ms/step - loss: 375362464.0000 - val_loss: 5601.9517 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11112158.0000 - val_loss: 19.8341 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2988284.7500 - val_loss: 20.3201 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1678561.2500 - val_loss: 19.5972 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1149917.3750 - val_loss: 19.6074 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 900163.6250 - val_loss: 19.6201 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 731520.1250 - val_loss: 19.6298 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 596954.8750 - val_loss: 19.6109 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 506249.8438 - val_loss: 19.5955 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 449338.3438 - val_loss: 19.6119 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 371083.6875 - val_loss: 19.6020 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 319310.7500 - val_loss: 19.6115 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 274401.4062 - val_loss: 19.4839 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 244286.7344 - val_loss: 19.1393 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 224008.0000 - val_loss: 19.0348 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 195626.7188 - val_loss: 18.2182 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 176894.9688 - val_loss: 17.7413 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 158555.2812 - val_loss: 17.1825 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 147749.3594 - val_loss: 17.4909 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 127510.4766 - val_loss: 16.1166 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 118563.4453 - val_loss: 16.9615 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 99213.8359 - val_loss: 18.9363 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 74362.5859 - val_loss: 19.6384 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 48364.1211 - val_loss: 19.6369 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 34965.0586 - val_loss: 19.6351 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 29124.6875 - val_loss: 19.6332 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 23318.7734 - val_loss: 19.6311 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 21197.9258 - val_loss: 19.5524 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 20099.7305 - val_loss: 18.3843 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 20341.3633 - val_loss: 17.0400 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 19256.3750 - val_loss: 14.2299 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 17946.4688 - val_loss: 21.8380 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 16623.4941 - val_loss: 18.8896 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 15716.4854 - val_loss: 17.5968 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 15424.2861 - val_loss: 14.8761 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 16050.7744 - val_loss: 16.0625 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 14867.7861 - val_loss: 14.2348 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13717.0088 - val_loss: 16.5452 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13422.9502 - val_loss: 12.5736 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13512.8848 - val_loss: 16.1521 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12800.3496 - val_loss: 18.0144 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12605.4229 - val_loss: 19.4152 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13156.4814 - val_loss: 20.2099 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10742.8135 - val_loss: 22.8109 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11276.6387 - val_loss: 18.9300 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11472.2490 - val_loss: 19.5689 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10339.2715 - val_loss: 19.5630 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10875.0879 - val_loss: 19.5571 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10556.6182 - val_loss: 19.5497 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11255.3652 - val_loss: 17.7889 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9632.8047 - val_loss: 19.0619 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 8871.8232 - val_loss: 14.7995 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9368.1523 - val_loss: 19.5263 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9038.0830 - val_loss: 19.4988 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7839.3462 - val_loss: 19.5114 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 8303.1045 - val_loss: 19.5030 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7887.0723 - val_loss: 19.4803 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7314.8564 - val_loss: 19.4628 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 8013.0703 - val_loss: 19.4773 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7103.2358 - val_loss: 19.4682 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6667.9912 - val_loss: 19.2811 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7328.7070 - val_loss: 19.4482 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6412.8809 - val_loss: 19.4369 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5827.0879 - val_loss: 19.4255 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6190.1533 - val_loss: 19.4142 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5493.9341 - val_loss: 19.3865 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5555.1162 - val_loss: 19.3897 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5316.6060 - val_loss: 19.3757 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6013.3809 - val_loss: 19.3611 - lr: 0.0010\n",
            "Epoch 70/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5421.4731 - val_loss: 19.3466 - lr: 0.0010\n",
            "Epoch 71/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4849.3760 - val_loss: 19.3322 - lr: 0.0010\n",
            "Epoch 72/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5195.4116 - val_loss: 19.3154 - lr: 0.0010\n",
            "Epoch 73/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4055.6562 - val_loss: 19.2978 - lr: 0.0010\n",
            "Epoch 74/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4856.1143 - val_loss: 19.2801 - lr: 0.0010\n",
            "Epoch 75/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4362.2886 - val_loss: 19.2608 - lr: 0.0010\n",
            "Epoch 76/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4534.6929 - val_loss: 19.2436 - lr: 0.0010\n",
            "Epoch 77/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4024.3848 - val_loss: 19.2229 - lr: 0.0010\n",
            "Epoch 78/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3997.9036 - val_loss: 19.2024 - lr: 0.0010\n",
            "Epoch 79/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4014.6821 - val_loss: 19.1820 - lr: 0.0010\n",
            "Epoch 80/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4020.6194 - val_loss: 19.1604 - lr: 0.0010\n",
            "Epoch 81/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3449.3875 - val_loss: 19.1362 - lr: 0.0010\n",
            "Epoch 82/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3666.3450 - val_loss: 19.1119 - lr: 0.0010\n",
            "Epoch 83/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3451.7332 - val_loss: 19.0878 - lr: 0.0010\n",
            "Epoch 84/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3156.5771 - val_loss: 19.0357 - lr: 0.0010\n",
            "Epoch 85/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3439.3420 - val_loss: 18.3285 - lr: 0.0010\n",
            "Epoch 86/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2946.1604 - val_loss: 14.9780 - lr: 0.0010\n",
            "Epoch 87/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3292.2869 - val_loss: 12.7749 - lr: 0.0010\n",
            "Epoch 88/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2869.8020 - val_loss: 11.3985 - lr: 0.0010\n",
            "Epoch 89/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3414.2051 - val_loss: 10.5026 - lr: 0.0010\n",
            "Epoch 90/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2763.4414 - val_loss: 10.1102 - lr: 0.0010\n",
            "Epoch 91/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2618.0059 - val_loss: 8.3534 - lr: 0.0010\n",
            "Epoch 92/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2403.2976 - val_loss: 8.8359 - lr: 0.0010\n",
            "Epoch 93/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2133.1582 - val_loss: 10.4350 - lr: 0.0010\n",
            "Epoch 94/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2256.6907 - val_loss: 7.8437 - lr: 0.0010\n",
            "Epoch 95/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2249.3665 - val_loss: 8.6998 - lr: 0.0010\n",
            "Epoch 96/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2056.8269 - val_loss: 9.2965 - lr: 0.0010\n",
            "Epoch 97/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2046.3649 - val_loss: 9.2581 - lr: 0.0010\n",
            "Epoch 98/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1880.0417 - val_loss: 8.3735 - lr: 0.0010\n",
            "Epoch 99/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2335.1255 - val_loss: 5.2198 - lr: 0.0010\n",
            "Epoch 100/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1723.0409 - val_loss: 9.4367 - lr: 0.0010\n",
            "Epoch 101/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1765.0045 - val_loss: 8.0753 - lr: 0.0010\n",
            "Epoch 102/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1638.2548 - val_loss: 30.3308 - lr: 0.0010\n",
            "Epoch 103/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2154.1375 - val_loss: 8.3538 - lr: 0.0010\n",
            "Epoch 104/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1697.9795 - val_loss: 8.7713 - lr: 0.0010\n",
            "Epoch 105/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1475.3893 - val_loss: 8.2715 - lr: 0.0010\n",
            "Epoch 106/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1426.6359 - val_loss: 52.8753 - lr: 0.0010\n",
            "Epoch 107/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1480.7202 - val_loss: 11.1732 - lr: 0.0010\n",
            "Epoch 108/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1550.7634 - val_loss: 8.4653 - lr: 0.0010\n",
            "Epoch 109/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1439.5808 - val_loss: 8.3411 - lr: 0.0010\n",
            "Epoch 110/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1182.8998 - val_loss: 6.8596 - lr: 0.0010\n",
            "Epoch 111/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1252.1487 - val_loss: 11.6532 - lr: 0.0010\n",
            "Epoch 112/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1064.9751 - val_loss: 9.7057 - lr: 0.0010\n",
            "Epoch 113/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 986.5560 - val_loss: 8.3524 - lr: 0.0010\n",
            "Epoch 114/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 818.6644 - val_loss: 12.9414 - lr: 0.0010\n",
            "Epoch 115/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1015.4670 - val_loss: 11.3432 - lr: 0.0010\n",
            "Epoch 116/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 857.2465 - val_loss: 10.0295 - lr: 0.0010\n",
            "Epoch 117/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 949.2403 - val_loss: 7.4764 - lr: 0.0010\n",
            "Epoch 118/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 841.8507 - val_loss: 5.8947 - lr: 0.0010\n",
            "Epoch 119/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 696.7091 - val_loss: 9.4831 - lr: 0.0010\n",
            "Epoch 120/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 828.0294 - val_loss: 6.9683 - lr: 0.0010\n",
            "Epoch 121/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 842.3337 - val_loss: 13.5347 - lr: 0.0010\n",
            "Epoch 122/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 837.7695 - val_loss: 12.4968 - lr: 0.0010\n",
            "Epoch 123/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 623.0227 - val_loss: 10.0539 - lr: 0.0010\n",
            "Epoch 124/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 778.0915 - val_loss: 8.6417 - lr: 0.0010\n",
            "Epoch 125/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 651.2606 - val_loss: 8.6313 - lr: 0.0010\n",
            "Epoch 126/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 539.9113 - val_loss: 12.6567 - lr: 0.0010\n",
            "Epoch 127/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 595.4330 - val_loss: 9.4064 - lr: 0.0010\n",
            "Epoch 128/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 444.7044 - val_loss: 8.4005 - lr: 0.0010\n",
            "Epoch 129/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 505.3209 - val_loss: 14.4590 - lr: 0.0010\n",
            "Epoch 130/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 438.7375 - val_loss: 11.1298 - lr: 0.0010\n",
            "Epoch 131/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 429.5309 - val_loss: 8.7132 - lr: 0.0010\n",
            "Epoch 132/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 421.5429 - val_loss: 7.2933 - lr: 0.0010\n",
            "Epoch 133/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 461.8954 - val_loss: 10.6446 - lr: 0.0010\n",
            "Epoch 134/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 373.1618 - val_loss: 5.0414 - lr: 0.0010\n",
            "Epoch 135/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 339.5249 - val_loss: 8.7169 - lr: 0.0010\n",
            "Epoch 136/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 333.9766 - val_loss: 9.5363 - lr: 0.0010\n",
            "Epoch 137/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 400.6938 - val_loss: 9.2050 - lr: 0.0010\n",
            "Epoch 138/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 368.7015 - val_loss: 13.1949 - lr: 0.0010\n",
            "Epoch 139/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 309.3472 - val_loss: 10.1749 - lr: 0.0010\n",
            "Epoch 140/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 165.0214 - val_loss: 6.8223 - lr: 0.0010\n",
            "Epoch 141/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 292.4820 - val_loss: 9.3952 - lr: 0.0010\n",
            "Epoch 142/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 195.8717 - val_loss: 14.3725 - lr: 0.0010\n",
            "Epoch 143/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 146.2352 - val_loss: 14.1244 - lr: 0.0010\n",
            "Epoch 144/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 127.6795 - val_loss: 11.4902 - lr: 0.0010\n",
            "Epoch 145/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 115.4204 - val_loss: 8.0229 - lr: 0.0010\n",
            "Epoch 146/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 125.9591 - val_loss: 8.8953 - lr: 0.0010\n",
            "Epoch 147/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 182.5528 - val_loss: 13.3012 - lr: 0.0010\n",
            "Epoch 148/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 117.1662 - val_loss: 13.0406 - lr: 0.0010\n",
            "Epoch 149/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 67.4958 - val_loss: 12.7732 - lr: 0.0010\n",
            "Epoch 150/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 99.0525 - val_loss: 12.5094 - lr: 0.0010\n",
            "Epoch 151/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 62.9806 - val_loss: 12.2488 - lr: 0.0010\n",
            "Epoch 152/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 81.5223 - val_loss: 11.9854 - lr: 0.0010\n",
            "Epoch 153/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 51.1515 - val_loss: 10.8680 - lr: 0.0010\n",
            "Epoch 154/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 50.1544 - val_loss: 7.6550 - lr: 0.0010\n",
            "Epoch 155/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 52.9818 - val_loss: 11.2603 - lr: 0.0010\n",
            "Epoch 156/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 77.7074 - val_loss: 9.8817 - lr: 0.0010\n",
            "Epoch 157/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 60.2965 - val_loss: 10.7857 - lr: 0.0010\n",
            "Epoch 158/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 32.1762 - val_loss: 10.4806 - lr: 0.0010\n",
            "Epoch 159/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 36.4753 - val_loss: 10.1755 - lr: 0.0010\n",
            "Epoch 160/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 31.9055 - val_loss: 9.8748 - lr: 0.0010\n",
            "Epoch 161/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 35.9322 - val_loss: 9.5787 - lr: 0.0010\n",
            "Epoch 162/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 36.4226 - val_loss: 9.2877 - lr: 0.0010\n",
            "Epoch 163/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 47.7843 - val_loss: 9.0028 - lr: 0.0010\n",
            "Epoch 164/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 25.4176 - val_loss: 8.7244 - lr: 0.0010\n",
            "Epoch 165/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 37.2694 - val_loss: 8.4539 - lr: 0.0010\n",
            "Epoch 166/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 24.2129 - val_loss: 8.1920 - lr: 0.0010\n",
            "Epoch 167/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 44.5467 - val_loss: 7.9383 - lr: 0.0010\n",
            "Epoch 168/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 21.4229 - val_loss: 7.6955 - lr: 0.0010\n",
            "Epoch 169/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 27.0409 - val_loss: 7.3791 - lr: 0.0010\n",
            "Epoch 170/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 28.6860 - val_loss: 7.2355 - lr: 0.0010\n",
            "Epoch 171/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10.7680 - val_loss: 6.9660 - lr: 0.0010\n",
            "Epoch 172/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 20.4010 - val_loss: 6.7037 - lr: 0.0010\n",
            "Epoch 173/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12.3853 - val_loss: 6.4491 - lr: 0.0010\n",
            "Epoch 174/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9.9176 - val_loss: 6.2020 - lr: 0.0010\n",
            "Epoch 175/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11.0252 - val_loss: 5.9632 - lr: 0.0010\n",
            "Epoch 176/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11.5745 - val_loss: 5.7296 - lr: 0.0010\n",
            "Epoch 177/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 19.1369 - val_loss: 5.5028 - lr: 0.0010\n",
            "Epoch 178/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 35.7163 - val_loss: 5.2838 - lr: 0.0010\n",
            "Epoch 179/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 17.4336 - val_loss: 5.0731 - lr: 0.0010\n",
            "Epoch 180/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 14.1955 - val_loss: 4.8685 - lr: 0.0010\n",
            "Epoch 181/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.6160 - val_loss: 4.6714 - lr: 0.0010\n",
            "Epoch 182/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 11.3903 - val_loss: 4.4807 - lr: 0.0010\n",
            "Epoch 183/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.2677 - val_loss: 4.2968 - lr: 0.0010\n",
            "Epoch 184/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 10.8116 - val_loss: 4.1186 - lr: 0.0010\n",
            "Epoch 185/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12.6384 - val_loss: 3.9481 - lr: 0.0010\n",
            "Epoch 186/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9.2083 - val_loss: 3.7846 - lr: 0.0010\n",
            "Epoch 187/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.7904 - val_loss: 3.6247 - lr: 0.0010\n",
            "Epoch 188/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 17.3306 - val_loss: 3.4714 - lr: 0.0010\n",
            "Epoch 189/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.8823 - val_loss: 3.3252 - lr: 0.0010\n",
            "Epoch 190/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13.0967 - val_loss: 3.1860 - lr: 0.0010\n",
            "Epoch 191/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 226.4327 - val_loss: 3.0540 - lr: 0.0010\n",
            "Epoch 192/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.9003 - val_loss: 2.9282 - lr: 0.0010\n",
            "Epoch 193/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.5490 - val_loss: 2.8087 - lr: 0.0010\n",
            "Epoch 194/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 21.2574 - val_loss: 2.6957 - lr: 0.0010\n",
            "Epoch 195/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.2627 - val_loss: 2.5888 - lr: 0.0010\n",
            "Epoch 196/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12.8602 - val_loss: 2.4879 - lr: 0.0010\n",
            "Epoch 197/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.1086 - val_loss: 2.3929 - lr: 0.0010\n",
            "Epoch 198/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.1500 - val_loss: 2.3035 - lr: 0.0010\n",
            "Epoch 199/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.9385 - val_loss: 2.2196 - lr: 0.0010\n",
            "Epoch 200/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.6545 - val_loss: 2.1411 - lr: 0.0010\n",
            "Epoch 201/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.8892 - val_loss: 2.0677 - lr: 0.0010\n",
            "Epoch 202/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.0064 - val_loss: 1.9993 - lr: 0.0010\n",
            "Epoch 203/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.7873 - val_loss: 1.9358 - lr: 0.0010\n",
            "Epoch 204/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.8684 - val_loss: 1.8769 - lr: 0.0010\n",
            "Epoch 205/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.3292 - val_loss: 1.8225 - lr: 0.0010\n",
            "Epoch 206/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.9927 - val_loss: 1.7725 - lr: 0.0010\n",
            "Epoch 207/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.4823 - val_loss: 1.7265 - lr: 0.0010\n",
            "Epoch 208/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.6421 - val_loss: 1.6842 - lr: 0.0010\n",
            "Epoch 209/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.7600 - val_loss: 1.6456 - lr: 0.0010\n",
            "Epoch 210/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.9346 - val_loss: 1.6104 - lr: 0.0010\n",
            "Epoch 211/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.0840 - val_loss: 1.5786 - lr: 0.0010\n",
            "Epoch 212/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.5106 - val_loss: 1.5498 - lr: 0.0010\n",
            "Epoch 213/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.4644 - val_loss: 1.5238 - lr: 0.0010\n",
            "Epoch 214/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6.2669 - val_loss: 1.5007 - lr: 0.0010\n",
            "Epoch 215/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.9400 - val_loss: 1.4801 - lr: 0.0010\n",
            "Epoch 216/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.9166 - val_loss: 1.4620 - lr: 0.0010\n",
            "Epoch 217/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.4328 - val_loss: 1.4461 - lr: 0.0010\n",
            "Epoch 218/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.8907 - val_loss: 1.4322 - lr: 0.0010\n",
            "Epoch 219/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 24.2691 - val_loss: 1.4200 - lr: 0.0010\n",
            "Epoch 220/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 31.4781 - val_loss: 1.4096 - lr: 0.0010\n",
            "Epoch 221/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 12.4467 - val_loss: 1.4008 - lr: 0.0010\n",
            "Epoch 222/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.4606 - val_loss: 1.3933 - lr: 0.0010\n",
            "Epoch 223/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.1569 - val_loss: 1.3870 - lr: 0.0010\n",
            "Epoch 224/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.2599 - val_loss: 1.3816 - lr: 0.0010\n",
            "Epoch 225/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6724 - val_loss: 1.3772 - lr: 0.0010\n",
            "Epoch 226/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.2657 - val_loss: 1.3736 - lr: 0.0010\n",
            "Epoch 227/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.0913 - val_loss: 1.3707 - lr: 0.0010\n",
            "Epoch 228/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.7088 - val_loss: 1.3684 - lr: 0.0010\n",
            "Epoch 229/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 47.1536 - val_loss: 1.3664 - lr: 0.0010\n",
            "Epoch 230/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.6812 - val_loss: 1.3650 - lr: 0.0010\n",
            "Epoch 231/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7687 - val_loss: 1.3639 - lr: 0.0010\n",
            "Epoch 232/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.1564 - val_loss: 1.3630 - lr: 0.0010\n",
            "Epoch 233/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5934 - val_loss: 1.3624 - lr: 0.0010\n",
            "Epoch 234/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.9588 - val_loss: 1.3619 - lr: 0.0010\n",
            "Epoch 235/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7578 - val_loss: 1.3616 - lr: 0.0010\n",
            "Epoch 236/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.8278 - val_loss: 1.3613 - lr: 0.0010\n",
            "Epoch 237/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6506 - val_loss: 1.3611 - lr: 0.0010\n",
            "Epoch 238/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 19.0682 - val_loss: 1.3610 - lr: 0.0010\n",
            "Epoch 239/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.8026 - val_loss: 1.3609 - lr: 0.0010\n",
            "Epoch 240/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7794 - val_loss: 1.3608 - lr: 0.0010\n",
            "Epoch 241/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6026 - val_loss: 1.3608 - lr: 0.0010\n",
            "Epoch 242/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5410 - val_loss: 1.3608 - lr: 0.0010\n",
            "Epoch 243/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5248 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 244/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 6.3271 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 245/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.8132 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 246/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.2611 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 247/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4826 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 248/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 9.6093 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 249/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4708 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 250/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.3218 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 251/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.6482 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 252/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.2688 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 253/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.2751 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 254/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4867 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 255/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6219 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 256/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4655 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 257/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.6121 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 258/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5049 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 259/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4315 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 260/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7525 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 261/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.0662 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 262/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7418 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 263/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7398 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 264/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5022 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 265/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5189 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 266/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3969 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 267/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7609 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 268/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5348 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 269/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4029 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 270/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.7144 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 271/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 4.1449 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 272/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5425 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 273/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4324 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 274/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.9137 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 275/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4817 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 276/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4288 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 277/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4590 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 278/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.8666 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 279/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 13.8671 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 280/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 7.6214 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 281/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6182 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 282/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.5743 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 283/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4498 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 284/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 3.9143 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 285/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4786 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 286/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.9587 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 287/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.6403 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 288/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4089 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 289/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3821 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 290/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3714 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 291/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 5.1177 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 292/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4394 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 293/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4164 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 294/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3891 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 295/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4202 - val_loss: 1.3607 - lr: 0.0010\n",
            "Epoch 296/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.5442 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 297/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4337 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 298/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.7859 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 299/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4037 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 300/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3884 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 301/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4339 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 302/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.4022 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 303/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 2.1840 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "Epoch 304/500\n",
            "82/82 [==============================] - 1s 8ms/step - loss: 1.3952 - val_loss: 1.3607 - lr: 2.0000e-04\n",
            "CPU times: user 5min 51s, sys: 21 s, total: 6min 12s\n",
            "Wall time: 3min 22s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluar el modelo\n",
        "y_true = ratings_val\n",
        "predictions = model.predict([user_data_val, item_data_val])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhuwylfbf26k",
        "outputId": "779a09b2-2dd8-4e80-be15-bd89e12be0b3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43518/43518 [==============================] - 69s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CmYiE_PhkU66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4b1d8b3-5eb7-4c59-dd42-0bdc12435388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.3605828830348343\n",
            "RMSE: 1.166440261237083\n",
            "MAE: 0.9251185466427577\n",
            "MAPE: 38.36791751953778%\n"
          ]
        }
      ],
      "source": [
        "# MSE y RMSE\n",
        "mse = mean_squared_error(y_true, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# MAE\n",
        "mae = mean_absolute_error(y_true, predictions)\n",
        "\n",
        "# Funciones MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
        "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
        "\n",
        "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
        "    mape_sum = 0\n",
        "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "\n",
        "        batch_y_true = y_true[start_idx:end_idx]\n",
        "        batch_predictions = predictions[start_idx:end_idx]\n",
        "\n",
        "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
        "\n",
        "    return mape_sum / num_batches\n",
        "\n",
        "# Luego llamas a la función\n",
        "mape = compute_mape_by_batch(y_true, predictions)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'MAE: {mae}')\n",
        "print(f'MAPE: {mape}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTOAZeGrujrU"
      },
      "source": [
        "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa y Embedding con Keras:\n",
        "\n",
        "Otra enfoque popular para filtrado colaborativo es utilizar redes neuronales multicapa, que esencialmente aprenden características no lineales de los datos. Vamos a construir un modelo que fusiona los embeddings de usuarios y artículos (por ejemplo, películas) en una red neuronal densa.\n",
        "\n",
        "Este enfoque, que combina embeddings y redes neuronales multicapa, puede capturar interacciones más complejas y no lineales entre usuarios y artículos. Sin embargo, es crucial prestar atención al sobreajuste y asegurarse de que el modelo no esté simplemente memorizando los datos. Por lo tanto, es recomendable emplear técnicas de regularización, ajustar hiperparámetros y validar el rendimiento con un conjunto de datos de validación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3Rzzhtfl7nK"
      },
      "source": [
        "Funcionamiento:\n",
        "\n",
        "Combina lo mejor de ambos mundos. Primero, se utilizan embeddings para convertir ítems y usuarios en representaciones vectoriales densas. Luego, estas representaciones se pasan a través de una red neuronal para hacer la predicción.\n",
        "\n",
        "El proceso generalmente comienza con capas de embedding que convierten identificadores de usuarios e ítems en vectores. Estos vectores luego se pasan a través de capas densas para obtener la predicción final.\n",
        "\n",
        "Ventajas:\n",
        "\n",
        "Capacidad de capturar patrones subyacentes en los datos mediante embeddings y modelar interacciones no lineales mediante la red neuronal.\n",
        "\n",
        "Flexibilidad para incorporar características adicionales.\n",
        "\n",
        "Potencialmente más preciso que cualquiera de los otros dos métodos por separado.\n",
        "\n",
        "Desventajas:\n",
        "\n",
        "Mayor complejidad en el modelado y entrenamiento.\n",
        "\n",
        "Mayor costo computacional.\n",
        "\n",
        "Requiere un ajuste más cuidadoso y riesgo de sobreajuste si no se gestiona adecuadamente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uY_n_5u4ymE8"
      },
      "source": [
        "Regularización L2: Agregar una regularización L2 a las capas de embedding y densas para prevenir el sobreajuste.\n",
        "\n",
        "Early Stopping: Añadir una callback de early stopping para detener el entrenamiento cuando no haya mejora en el conjunto de validación durante un cierto número de épocas.\n",
        "\n",
        "Ajuste de la tasa de aprendizaje: Utilizar la callback ReduceLROnPlateau para reducir la tasa de aprendizaje cuando no haya mejora en el conjunto de validación.\n",
        "\n",
        "Métricas adicionales: Puedes monitorizar otras métricas como el error absoluto medio (MAE) durante el entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "zVRB5utBuqzB"
      },
      "outputs": [],
      "source": [
        "# Convertir reviewerID y asin a índices numéricos secuenciales\n",
        "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
        "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
        "\n",
        "# Número de usuarios e ítems\n",
        "n_users = df['user_id'].nunique()\n",
        "n_items = df['item_id'].nunique()\n",
        "\n",
        "# Hiperparámetros\n",
        "n_latent_factors = 50\n",
        "hidden_units = [128, 64]\n",
        "dropout_rate = 0.2\n",
        "l2_reg = 1e-4\n",
        "\n",
        "# Arquitectura del modelo con regularización L2\n",
        "user_embedding = Embedding(n_users, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='user_embedding')(user_input)\n",
        "item_embedding = Embedding(n_items, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='item_embedding')(item_input)\n",
        "\n",
        "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
        "item_vec = Flatten(name='flatten_items')(item_embedding)\n",
        "concat = Concatenate()([user_vec, item_vec])\n",
        "dense = concat\n",
        "for units in hidden_units:\n",
        "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
        "    dense = Dropout(dropout_rate)(dense)\n",
        "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)\n",
        "model = Model(inputs=[user_input, item_input], outputs=output)\n",
        "\n",
        "model.compile(optimizer=Adam(0.001), loss='mean_squared_error', metrics=['mae'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "3I7zFMJBjTZ7"
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "XJfWiirYxHwk"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento\n",
        "user_data = df['user_id'].values\n",
        "item_data = df['item_id'].values\n",
        "rating_data = df['overall'].values\n",
        "\n",
        "# Usando validation_split, divide los datos\n",
        "train_size = int(0.7 * len(user_data))\n",
        "user_data_train, user_data_val = user_data[:train_size], user_data[train_size:]\n",
        "item_data_train, item_data_val = item_data[:train_size], item_data[train_size:]\n",
        "y_true = rating_data[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "history = model.fit([user_data_train, item_data_train], rating_data[:train_size],\n",
        "                    epochs=500,validation_data=([user_data_val, item_data_val], y_true),\n",
        "                    batch_size=40000,verbose=1,callbacks=[early_stop, reduce_lr])"
      ],
      "metadata": {
        "id": "oXQNam6Bf-dT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad5368b1-3536-441d-ea67-fd085cceaa14"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "82/82 [==============================] - 6s 49ms/step - loss: 10.2316 - mae: 2.4053 - val_loss: 5.1458 - val_mae: 2.0052 - lr: 0.0010\n",
            "Epoch 2/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.8189 - mae: 0.9057 - val_loss: 3.9887 - val_mae: 1.7391 - lr: 0.0010\n",
            "Epoch 3/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.6130 - mae: 0.8550 - val_loss: 3.1395 - val_mae: 1.5108 - lr: 0.0010\n",
            "Epoch 4/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.4873 - mae: 0.8270 - val_loss: 2.4850 - val_mae: 1.3118 - lr: 0.0010\n",
            "Epoch 5/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.3855 - mae: 0.8021 - val_loss: 2.0791 - val_mae: 1.1533 - lr: 0.0010\n",
            "Epoch 6/500\n",
            "82/82 [==============================] - 3s 43ms/step - loss: 1.3243 - mae: 0.7847 - val_loss: 1.9432 - val_mae: 1.0651 - lr: 0.0010\n",
            "Epoch 7/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.2835 - mae: 0.7705 - val_loss: 1.8058 - val_mae: 0.9994 - lr: 0.0010\n",
            "Epoch 8/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.2250 - mae: 0.7549 - val_loss: 1.7950 - val_mae: 0.9750 - lr: 0.0010\n",
            "Epoch 9/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.1918 - mae: 0.7449 - val_loss: 1.7432 - val_mae: 0.9451 - lr: 0.0010\n",
            "Epoch 10/500\n",
            "82/82 [==============================] - 3s 43ms/step - loss: 1.1539 - mae: 0.7352 - val_loss: 1.7369 - val_mae: 0.9337 - lr: 0.0010\n",
            "Epoch 11/500\n",
            "82/82 [==============================] - 3s 43ms/step - loss: 1.1197 - mae: 0.7256 - val_loss: 1.7030 - val_mae: 0.9221 - lr: 0.0010\n",
            "Epoch 12/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 1.0909 - mae: 0.7179 - val_loss: 1.7060 - val_mae: 0.9186 - lr: 0.0010\n",
            "Epoch 13/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.0741 - mae: 0.7097 - val_loss: 1.6917 - val_mae: 0.9062 - lr: 0.0010\n",
            "Epoch 14/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 1.0506 - mae: 0.6982 - val_loss: 1.6949 - val_mae: 0.8992 - lr: 0.0010\n",
            "Epoch 15/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.0301 - mae: 0.6885 - val_loss: 1.6841 - val_mae: 0.8907 - lr: 0.0010\n",
            "Epoch 16/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 1.0083 - mae: 0.6803 - val_loss: 1.6671 - val_mae: 0.8893 - lr: 0.0010\n",
            "Epoch 17/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 0.9897 - mae: 0.6739 - val_loss: 1.6618 - val_mae: 0.8788 - lr: 0.0010\n",
            "Epoch 18/500\n",
            "82/82 [==============================] - 3s 42ms/step - loss: 0.9761 - mae: 0.6687 - val_loss: 1.6600 - val_mae: 0.8793 - lr: 0.0010\n",
            "Epoch 19/500\n",
            "82/82 [==============================] - 3s 43ms/step - loss: 0.9656 - mae: 0.6638 - val_loss: 1.6544 - val_mae: 0.8743 - lr: 0.0010\n",
            "Epoch 20/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9550 - mae: 0.6588 - val_loss: 1.6623 - val_mae: 0.8708 - lr: 0.0010\n",
            "Epoch 21/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9515 - mae: 0.6550 - val_loss: 1.6729 - val_mae: 0.8690 - lr: 0.0010\n",
            "Epoch 22/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9469 - mae: 0.6514 - val_loss: 1.6714 - val_mae: 0.8692 - lr: 0.0010\n",
            "Epoch 23/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9369 - mae: 0.6474 - val_loss: 1.6708 - val_mae: 0.8640 - lr: 0.0010\n",
            "Epoch 24/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9287 - mae: 0.6436 - val_loss: 1.6787 - val_mae: 0.8660 - lr: 0.0010\n",
            "Epoch 25/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9276 - mae: 0.6412 - val_loss: 1.6856 - val_mae: 0.8630 - lr: 0.0010\n",
            "Epoch 26/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9215 - mae: 0.6379 - val_loss: 1.6768 - val_mae: 0.8620 - lr: 0.0010\n",
            "Epoch 27/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9094 - mae: 0.6340 - val_loss: 1.6767 - val_mae: 0.8615 - lr: 0.0010\n",
            "Epoch 28/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.9060 - mae: 0.6318 - val_loss: 1.6845 - val_mae: 0.8611 - lr: 0.0010\n",
            "Epoch 29/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8984 - mae: 0.6284 - val_loss: 1.6738 - val_mae: 0.8583 - lr: 0.0010\n",
            "Epoch 30/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8838 - mae: 0.6241 - val_loss: 1.6768 - val_mae: 0.8603 - lr: 0.0010\n",
            "Epoch 31/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8833 - mae: 0.6224 - val_loss: 1.6773 - val_mae: 0.8617 - lr: 0.0010\n",
            "Epoch 32/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8745 - mae: 0.6182 - val_loss: 1.6841 - val_mae: 0.8620 - lr: 0.0010\n",
            "Epoch 33/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8700 - mae: 0.6152 - val_loss: 1.6852 - val_mae: 0.8572 - lr: 0.0010\n",
            "Epoch 34/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8605 - mae: 0.6106 - val_loss: 1.6777 - val_mae: 0.8657 - lr: 0.0010\n",
            "Epoch 35/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8502 - mae: 0.6064 - val_loss: 1.6703 - val_mae: 0.8659 - lr: 0.0010\n",
            "Epoch 36/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8469 - mae: 0.6039 - val_loss: 1.6922 - val_mae: 0.8673 - lr: 0.0010\n",
            "Epoch 37/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8416 - mae: 0.5991 - val_loss: 1.6792 - val_mae: 0.8751 - lr: 0.0010\n",
            "Epoch 38/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8363 - mae: 0.5958 - val_loss: 1.6922 - val_mae: 0.8775 - lr: 0.0010\n",
            "Epoch 39/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8307 - mae: 0.5916 - val_loss: 1.6951 - val_mae: 0.8753 - lr: 0.0010\n",
            "Epoch 40/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8240 - mae: 0.5875 - val_loss: 1.7012 - val_mae: 0.8822 - lr: 0.0010\n",
            "Epoch 41/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8184 - mae: 0.5839 - val_loss: 1.7092 - val_mae: 0.8855 - lr: 0.0010\n",
            "Epoch 42/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8129 - mae: 0.5804 - val_loss: 1.7140 - val_mae: 0.8835 - lr: 0.0010\n",
            "Epoch 43/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8051 - mae: 0.5763 - val_loss: 1.7206 - val_mae: 0.8918 - lr: 0.0010\n",
            "Epoch 44/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.8025 - mae: 0.5732 - val_loss: 1.7284 - val_mae: 0.8975 - lr: 0.0010\n",
            "Epoch 45/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7934 - mae: 0.5691 - val_loss: 1.7297 - val_mae: 0.8994 - lr: 0.0010\n",
            "Epoch 46/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7875 - mae: 0.5658 - val_loss: 1.7424 - val_mae: 0.9026 - lr: 0.0010\n",
            "Epoch 47/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7829 - mae: 0.5623 - val_loss: 1.7532 - val_mae: 0.9088 - lr: 0.0010\n",
            "Epoch 48/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7756 - mae: 0.5585 - val_loss: 1.7597 - val_mae: 0.9063 - lr: 0.0010\n",
            "Epoch 49/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7718 - mae: 0.5558 - val_loss: 1.7651 - val_mae: 0.9070 - lr: 0.0010\n",
            "Epoch 50/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7633 - mae: 0.5521 - val_loss: 1.7706 - val_mae: 0.9121 - lr: 0.0010\n",
            "Epoch 51/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7595 - mae: 0.5491 - val_loss: 1.7806 - val_mae: 0.9253 - lr: 0.0010\n",
            "Epoch 52/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7525 - mae: 0.5454 - val_loss: 1.7843 - val_mae: 0.9206 - lr: 0.0010\n",
            "Epoch 53/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7482 - mae: 0.5427 - val_loss: 1.7964 - val_mae: 0.9293 - lr: 0.0010\n",
            "Epoch 54/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7418 - mae: 0.5396 - val_loss: 1.7962 - val_mae: 0.9261 - lr: 0.0010\n",
            "Epoch 55/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7358 - mae: 0.5369 - val_loss: 1.8119 - val_mae: 0.9361 - lr: 0.0010\n",
            "Epoch 56/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7318 - mae: 0.5337 - val_loss: 1.8139 - val_mae: 0.9397 - lr: 0.0010\n",
            "Epoch 57/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7243 - mae: 0.5310 - val_loss: 1.8207 - val_mae: 0.9391 - lr: 0.0010\n",
            "Epoch 58/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7213 - mae: 0.5278 - val_loss: 1.8201 - val_mae: 0.9404 - lr: 0.0010\n",
            "Epoch 59/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7149 - mae: 0.5248 - val_loss: 1.8388 - val_mae: 0.9456 - lr: 0.0010\n",
            "Epoch 60/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7130 - mae: 0.5222 - val_loss: 1.8374 - val_mae: 0.9514 - lr: 0.0010\n",
            "Epoch 61/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7070 - mae: 0.5195 - val_loss: 1.8476 - val_mae: 0.9504 - lr: 0.0010\n",
            "Epoch 62/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.7016 - mae: 0.5158 - val_loss: 1.8538 - val_mae: 0.9545 - lr: 0.0010\n",
            "Epoch 63/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6995 - mae: 0.5138 - val_loss: 1.8556 - val_mae: 0.9535 - lr: 0.0010\n",
            "Epoch 64/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6861 - mae: 0.5083 - val_loss: 1.8583 - val_mae: 0.9636 - lr: 0.0010\n",
            "Epoch 65/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6863 - mae: 0.5061 - val_loss: 1.8626 - val_mae: 0.9552 - lr: 0.0010\n",
            "Epoch 66/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6754 - mae: 0.5006 - val_loss: 1.8909 - val_mae: 0.9756 - lr: 0.0010\n",
            "Epoch 67/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6784 - mae: 0.4976 - val_loss: 1.8821 - val_mae: 0.9792 - lr: 0.0010\n",
            "Epoch 68/500\n",
            "82/82 [==============================] - 3s 38ms/step - loss: 0.6683 - mae: 0.4936 - val_loss: 1.8915 - val_mae: 0.9742 - lr: 0.0010\n",
            "Epoch 69/500\n",
            "82/82 [==============================] - 4s 43ms/step - loss: 0.6614 - mae: 0.4889 - val_loss: 1.8984 - val_mae: 0.9793 - lr: 0.0010\n",
            "CPU times: user 3min 35s, sys: 11.6 s, total: 3min 46s\n",
            "Wall time: 3min 44s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicciones\n",
        "predictions = model.predict([user_data_val, item_data_val])"
      ],
      "metadata": {
        "id": "2Off5YI9f-_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1d60bfc-920c-40a1-e93f-c3033c56d91f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43518/43518 [==============================] - 68s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "w1BY-Sd0xJtc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d48aa7f-7e7d-4c8e-889a-e22b394091e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.476765427193881\n",
            "RMSE: 1.2152223776716264\n",
            "MAE: 0.8742751385670862\n",
            "MAPE: 40.65795777215035%\n"
          ]
        }
      ],
      "source": [
        "# MSE y RMSE\n",
        "mse = mean_squared_error(y_true, predictions)\n",
        "rmse = np.sqrt(mse)\n",
        "\n",
        "# MAE\n",
        "mae = mean_absolute_error(y_true, predictions)\n",
        "\n",
        "# MAPE\n",
        "def mean_absolute_percentage_error(y_true, y_pred):\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
        "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
        "\n",
        "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
        "    mape_sum = 0\n",
        "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
        "\n",
        "    for i in range(num_batches):\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "\n",
        "        batch_y_true = y_true[start_idx:end_idx]\n",
        "        batch_predictions = predictions[start_idx:end_idx]\n",
        "\n",
        "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
        "\n",
        "    return mape_sum / num_batches\n",
        "\n",
        "# Luego llamas a la función\n",
        "mape = compute_mape_by_batch(y_true, predictions)\n",
        "\n",
        "print(f'MSE: {mse}')\n",
        "print(f'RMSE: {rmse}')\n",
        "print(f'MAE: {mae}')\n",
        "print(f'MAPE: {mape}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YopA3D2Sw8AO"
      },
      "source": [
        "Estas métricas te ayudarán a tener una mejor idea del desempeño de tu modelo. Por ejemplo:\n",
        "\n",
        "MSE y RMSE son útiles cuando quieres penalizar grandes errores.\n",
        "\n",
        "MAE te da una idea del error medio sin considerar la dirección del error.\n",
        "\n",
        "MAPE es útil cuando quieres representar el error en términos porcentuales.\n",
        "\n",
        "Para una evaluación completa, es recomendable utilizar un conjunto de validación aparte (es decir, no solo depender del validation_split). Esto asegura que estás evaluando el desempeño en datos que el modelo nunca ha visto durante el entrenamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dgWsjHFlB67"
      },
      "source": [
        "En resumen, la elección del tipo de sistema de recomendación dependerá de la naturaleza del conjunto de datos, las características disponibles, las capacidades computacionales y el tipo de relaciones o interacciones que se desean capturar. A menudo, un enfoque híbrido (como el sistema que combina embeddings y redes neuronales) ofrece un buen equilibrio entre precisión y eficiencia."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}