{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-SSaV-bRZZl"
   },
   "source": [
    "# Paso 1: Importar LibrerÃ­as de Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4mFYevMKu6y",
    "outputId": "dbca770d-5c86-4833-bcf7-6eb8d464a36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "     -------------------------------------- 772.0/772.0 kB 5.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.10.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "  Running setup.py install for scikit-surprise: started\n",
      "  Running setup.py install for scikit-surprise: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [76 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'surprise.prediction_algorithms' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'surprise.prediction_algorithms' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'surprise.prediction_algorithms' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'surprise.prediction_algorithms' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for scikit-surprise did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [78 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'surprise.prediction_algorithms' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'surprise.prediction_algorithms' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'surprise.prediction_algorithms' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'surprise.prediction_algorithms' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "scikit-surprise\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "47ItRxlORZ9i"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# IntegraciÃ³n con Google Colab\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Machine Learning: Preprocesamiento y divisiÃ³n de datos\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# AnÃ¡lisis de datos y manipulaciÃ³n\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# VisualizaciÃ³n de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilidades de fechas\n",
    "from datetime import datetime\n",
    "\n",
    "# IntegraciÃ³n con Google Colab\n",
    "from google.colab import drive\n",
    "\n",
    "# Machine Learning: Preprocesamiento y divisiÃ³n de datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine Learning: MÃ©tricas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Machine Learning: Modelado y validaciÃ³n\n",
    "from surprise import SVD, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Deep Learning: ConstrucciÃ³n de modelos y capas\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, Dot\n",
    "\n",
    "# Deep Learning: OptimizaciÃ³n y regularizaciÃ³n\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Deep Learning: Callbacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bHcBRlQYY4h",
    "outputId": "3097562f-a506-4b75-ce75-fb852ea21586"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "209f_YHQRaCi"
   },
   "source": [
    "# Paso 2: Lectura del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vIFqmMIRaIc",
    "outputId": "2b3e766d-6d31-487d-c78c-5a881888cd57"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tu DataFrame se llama df\n",
    "df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')  # Descomenta y proporciona la ruta si estÃ¡s cargando un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwnA5HnLr8r6",
    "outputId": "ce8c21ba-8e96-44a5-a56d-89bfc0c0f5f3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lMsQcyyS2NS"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Embedding con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ49x7CvmOfM"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Los embeddings son representaciones vectoriales densas y de baja dimensiÃ³n de Ã­tems y/o usuarios. Estas representaciones capturan relaciones semÃ¡nticas entre Ã­tems o entre usuarios.\n",
    "\n",
    "Un mÃ©todo popular para generar embeddings es la factorizaciÃ³n de matrices, como la descomposiciÃ³n en valores singulares (SVD). En el contexto de sistemas de recomendaciÃ³n, se busca factorizar la matriz de interacciones usuario-Ã­tem en dos matrices mÃ¡s pequeÃ±as (una para los usuarios y otra para los Ã­tems) cuyo producto aproximado reproduce la matriz original lo mejor posible.\n",
    "\n",
    "Una vez que se han obtenido los embeddings, la predicciÃ³n de una calificaciÃ³n o interacciÃ³n entre un usuario e Ã­tem se realiza tomando el producto escalar entre sus embeddings respectivos.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Simplicidad y eficiencia en tÃ©rminos computacionales.\n",
    "\n",
    "Puede manejar grandes conjuntos de datos debido a su naturaleza de baja dimensiÃ³n.\n",
    "\n",
    "Es efectivo para capturar patrones subyacentes en los datos.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "No tiene en cuenta caracterÃ­sticas adicionales de usuarios o Ã­tems.\n",
    "\n",
    "Dificultades para manejar nuevos Ã­tems o usuarios (problema de arranque en frÃ­o)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ482XODiA8T"
   },
   "source": [
    "El cÃ³digo ahora incluye regularizaciÃ³n en los embeddings, un callback para detener el entrenamiento si no hay mejoras (early stopping) y otro para reducir la tasa de aprendizaje si el error de validaciÃ³n no mejora (reduce learning rate on plateau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oQAn6BZT8ZB"
   },
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22AEmdhqU9vC"
   },
   "outputs": [],
   "source": [
    "# Crear un LabelEncoder para cada columna\n",
    "reviewerID_encoder = LabelEncoder()\n",
    "asin_encoder = LabelEncoder()\n",
    "\n",
    "# Ajustar y transformar las columnas\n",
    "data['reviewerID_encoded'] = reviewerID_encoder.fit_transform(data['reviewerID'])\n",
    "data['asin_encoded'] = asin_encoder.fit_transform(data['asin'])\n",
    "\n",
    "# NÃºmero de usuarios Ãºnicos y nÃºmero de Ã­tems Ãºnicos\n",
    "n_users = len(np.unique(data['reviewerID_encoded']))\n",
    "n_items = len(np.unique(data['asin_encoded']))\n",
    "\n",
    "# Dimensiones del embedding\n",
    "embedding_dim = 10\n",
    "\n",
    "# Entradas\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "\n",
    "# Embeddings con regularizaciÃ³n\n",
    "user_embedding = Embedding(n_users, embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)\n",
    "item_embedding = Embedding(n_items, embedding_dim, embeddings_regularizer=l2(1e-6))(item_input)\n",
    "\n",
    "# Producto punto para predecir la valoraciÃ³n/rating\n",
    "merged = Dot(axes=2)([user_embedding, item_embedding])\n",
    "merged = Flatten()(merged)\n",
    "\n",
    "# Modelo\n",
    "model = Model(inputs=[user_input, item_input], outputs=merged)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN3t1PGSYEzR"
   },
   "source": [
    "verbose=0: No muestra ninguna barra de progreso ni mÃ©tricas.\n",
    "\n",
    "verbose=1: Muestra una barra de progreso y actualiza las mÃ©tricas despuÃ©s de cada lote.\n",
    "\n",
    "verbose=2: Muestra las mÃ©tricas despuÃ©s de cada Ã©poca, pero no muestra la barra de progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3n02-vPg5wh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Crear los arrays con los datos\n",
    "user_ids = data['reviewerID_encoded'].values\n",
    "item_ids = data['asin_encoded'].values\n",
    "ratings = data['overall'].values\n",
    "\n",
    "# DivisiÃ³n en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# DivisiÃ³n del conjunto de entrenamiento en entrenamiento y validaciÃ³n\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYbYJfevgXpj"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOiSfVMfg5tw",
    "outputId": "e4487dec-c6a8-4c0f-ab19-1173f652f86e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "model.fit([user_ids_train, item_ids_train], ratings_train, validation_data=([user_ids_val, item_ids_val], ratings_val), epochs=500, batch_size=800000, verbose=1, callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnBHBOT1oel0",
    "outputId": "bd896c81-223c-46c7-b27e-1337332945ba"
   },
   "outputs": [],
   "source": [
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMJd_nt7g5h4",
    "outputId": "d42a83aa-d428-4fba-b224-0368db70236b"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo con el conjunto de prueba\n",
    "loss, mae = model.evaluate([user_ids_test, item_ids_test], ratings_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USnKRkm0KCdW",
    "outputId": "84e6c104-f1cc-4c14-bbfb-98c5317619af"
   },
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_true - predictions.flatten()) / y_true)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odgbIWArkJ2y"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBK_zPK7lvti"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Estos sistemas toman caracterÃ­sticas de los Ã­tems y/o usuarios y las pasan a travÃ©s de una o varias capas de neuronas para obtener una predicciÃ³n.\n",
    "Las redes neuronales son capaces de capturar interacciones no lineales entre caracterÃ­sticas, lo que las hace poderosas para tareas de modelado complejas.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de modelar relaciones no lineales.\n",
    "\n",
    "Flexibilidad para incorporar mÃºltiples fuentes de datos o caracterÃ­sticas.\n",
    "\n",
    "Puede manejar arranques en frÃ­o al incorporar caracterÃ­sticas de nuevos Ã­tems o usuarios.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor costo computacional en comparaciÃ³n con los sistemas basados Ãºnicamente en embeddings.\n",
    "\n",
    "Riesgo de sobreajuste si no se tiene un conjunto de datos lo suficientemente grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMnns-ENh_KJ"
   },
   "source": [
    "Ahora el cÃ³digo incluye regularizaciÃ³n en los embeddings, early stopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validaciÃ³n, y reducciÃ³n de la tasa de aprendizaje si el error en el conjunto de validaciÃ³n no mejora despuÃ©s de algunas Ã©pocas. Estas adiciones deberÃ­an ayudar a mejorar la capacidad de generalizaciÃ³n del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFgVZSeTkKGy"
   },
   "outputs": [],
   "source": [
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# NÃºmero de usuarios e Ã­tems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# HiperparÃ¡metros\n",
    "hidden_units = [128, 64, 32]\n",
    "dropout_rate = 0.2\n",
    "l2_reg = 1e-6\n",
    "\n",
    "# Arquitectura del modelo\n",
    "user_input = Input(shape=[1], name='user_input')\n",
    "item_input = Input(shape=[1], name='item_input')\n",
    "concat = Concatenate()([user_input, item_input])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)  # TambiÃ©n puedes aplicar regularizaciÃ³n L2 aquÃ­\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7uvl8VEhqdD"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blU4Q9akkVBc"
   },
   "outputs": [],
   "source": [
    "# Datos para entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# DivisiÃ³n en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# DivisiÃ³n del conjunto de entrenamiento en entrenamiento y validaciÃ³n\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGxLwUhzf32z",
    "outputId": "5860c511-44e1-477a-f298-59abaf192b61"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit([user_ids_train, item_ids_train], ratings_train,\n",
    "    epochs=500,validation_data=([user_ids_val, item_ids_val], ratings_val),\n",
    "    batch_size=800000,verbose=1,callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhuwylfbf26k",
    "outputId": "adefb079-ecbb-4e53-a2ef-7ed5bab1caca"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmYiE_PhkU66",
    "outputId": "d945c634-6546-409c-bfaa-c5d84496eaf2"
   },
   "outputs": [],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# Funciones MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la funciÃ³n\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTOAZeGrujrU"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa y Embedding con Keras:\n",
    "\n",
    "Otra enfoque popular para filtrado colaborativo es utilizar redes neuronales multicapa, que esencialmente aprenden caracterÃ­sticas no lineales de los datos. Vamos a construir un modelo que fusiona los embeddings de usuarios y artÃ­culos (por ejemplo, pelÃ­culas) en una red neuronal densa.\n",
    "\n",
    "Este enfoque, que combina embeddings y redes neuronales multicapa, puede capturar interacciones mÃ¡s complejas y no lineales entre usuarios y artÃ­culos. Sin embargo, es crucial prestar atenciÃ³n al sobreajuste y asegurarse de que el modelo no estÃ© simplemente memorizando los datos. Por lo tanto, es recomendable emplear tÃ©cnicas de regularizaciÃ³n, ajustar hiperparÃ¡metros y validar el rendimiento con un conjunto de datos de validaciÃ³n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Rzzhtfl7nK"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Combina lo mejor de ambos mundos. Primero, se utilizan embeddings para convertir Ã­tems y usuarios en representaciones vectoriales densas. Luego, estas representaciones se pasan a travÃ©s de una red neuronal para hacer la predicciÃ³n.\n",
    "\n",
    "El proceso generalmente comienza con capas de embedding que convierten identificadores de usuarios e Ã­tems en vectores. Estos vectores luego se pasan a travÃ©s de capas densas para obtener la predicciÃ³n final.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de capturar patrones subyacentes en los datos mediante embeddings y modelar interacciones no lineales mediante la red neuronal.\n",
    "\n",
    "Flexibilidad para incorporar caracterÃ­sticas adicionales.\n",
    "\n",
    "Potencialmente mÃ¡s preciso que cualquiera de los otros dos mÃ©todos por separado.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor complejidad en el modelado y entrenamiento.\n",
    "\n",
    "Mayor costo computacional.\n",
    "\n",
    "Requiere un ajuste mÃ¡s cuidadoso y riesgo de sobreajuste si no se gestiona adecuadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY_n_5u4ymE8"
   },
   "source": [
    "RegularizaciÃ³n L2: Agregar una regularizaciÃ³n L2 a las capas de embedding y densas para prevenir el sobreajuste.\n",
    "\n",
    "Early Stopping: AÃ±adir una callback de early stopping para detener el entrenamiento cuando no haya mejora en el conjunto de validaciÃ³n durante un cierto nÃºmero de Ã©pocas.\n",
    "\n",
    "Ajuste de la tasa de aprendizaje: Utilizar la callback ReduceLROnPlateau para reducir la tasa de aprendizaje cuando no haya mejora en el conjunto de validaciÃ³n.\n",
    "\n",
    "MÃ©tricas adicionales: Puedes monitorizar otras mÃ©tricas como el error absoluto medio (MAE) durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVRB5utBuqzB"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que el DataFrame se llama df\n",
    "df = df\n",
    "\n",
    "# Convertir reviewerID y asin a Ã­ndices numÃ©ricos secuenciales\n",
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# NÃºmero de usuarios e Ã­tems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# HiperparÃ¡metros\n",
    "n_latent_factors = 50\n",
    "hidden_units = [128, 64]\n",
    "dropout_rate = 0.2\n",
    "l2_reg = 1e-4\n",
    "\n",
    "# Arquitectura del modelo con regularizaciÃ³n L2\n",
    "user_embedding = Embedding(n_users, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_items, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='item_embedding')(item_input)\n",
    "\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "item_vec = Flatten(name='flatten_items')(item_embedding)\n",
    "concat = Concatenate()([user_vec, item_vec])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I7zFMJBjTZ7"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJfWiirYxHwk"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# DivisiÃ³n en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# DivisiÃ³n del conjunto de entrenamiento en entrenamiento y validaciÃ³n\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXQNam6Bf-dT",
    "outputId": "59b3dc80-4536-42ce-b04c-19e02ea66db6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit([user_ids_train, item_ids_train], ratings_train,\n",
    "                    epochs=500,validation_data=([user_ids_val, item_ids_val], ratings_val),\n",
    "                    batch_size=800000,verbose=1,callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Off5YI9f-_T",
    "outputId": "c960e6af-49d1-4740-a170-d7c87cf63483"
   },
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1BY-Sd0xJtc",
    "outputId": "47f064cb-4039-4759-ee42-9e2e75673021"
   },
   "outputs": [],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la funciÃ³n\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YopA3D2Sw8AO"
   },
   "source": [
    "Estas mÃ©tricas te ayudarÃ¡n a tener una mejor idea del desempeÃ±o de tu modelo. Por ejemplo:\n",
    "\n",
    "MSE y RMSE son Ãºtiles cuando quieres penalizar grandes errores.\n",
    "\n",
    "MAE te da una idea del error medio sin considerar la direcciÃ³n del error.\n",
    "\n",
    "MAPE es Ãºtil cuando quieres representar el error en tÃ©rminos porcentuales.\n",
    "\n",
    "Para una evaluaciÃ³n completa, es recomendable utilizar un conjunto de validaciÃ³n aparte (es decir, no solo depender del validation_split). Esto asegura que estÃ¡s evaluando el desempeÃ±o en datos que el modelo nunca ha visto durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dgWsjHFlB67"
   },
   "source": [
    "En resumen, la elecciÃ³n del tipo de sistema de recomendaciÃ³n dependerÃ¡ de la naturaleza del conjunto de datos, las caracterÃ­sticas disponibles, las capacidades computacionales y el tipo de relaciones o interacciones que se desean capturar. A menudo, un enfoque hÃ­brido (como el sistema que combina embeddings y redes neuronales) ofrece un buen equilibrio entre precisiÃ³n y eficiencia."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
