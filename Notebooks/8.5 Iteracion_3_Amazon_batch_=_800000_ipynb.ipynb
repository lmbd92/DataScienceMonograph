{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-SSaV-bRZZl"
   },
   "source": [
    "# Paso 1: Importar Librerías de Python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K4mFYevMKu6y",
    "outputId": "dbca770d-5c86-4833-bcf7-6eb8d464a36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-surprise\n",
      "  Downloading scikit-surprise-1.1.3.tar.gz (771 kB)\n",
      "     -------------------------------------- 772.0/772.0 kB 5.4 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: joblib>=1.0.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from scikit-surprise) (1.10.0)\n",
      "Building wheels for collected packages: scikit-surprise\n",
      "  Building wheel for scikit-surprise (setup.py): started\n",
      "  Building wheel for scikit-surprise (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for scikit-surprise\n",
      "Failed to build scikit-surprise\n",
      "Installing collected packages: scikit-surprise\n",
      "  Running setup.py install for scikit-surprise: started\n",
      "  Running setup.py install for scikit-surprise: finished with status 'error'\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [76 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'surprise.prediction_algorithms' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'surprise.prediction_algorithms' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'surprise.prediction_algorithms' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'surprise.prediction_algorithms' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Running setup.py install for scikit-surprise did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [78 lines of output]\n",
      "  running install\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\install.py:34: SetuptoolsDeprecationWarning: setup.py install is deprecated. Use build and pip and other standards-based tools.\n",
      "    warnings.warn(\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-310\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\command\\build_py.py:202: SetuptoolsDeprecationWarning:     Installing 'surprise.prediction_algorithms' as data is deprecated, please list it in `packages`.\n",
      "      !!\n",
      "  \n",
      "  \n",
      "      ############################\n",
      "      # Package would be ignored #\n",
      "      ############################\n",
      "      Python recognizes 'surprise.prediction_algorithms' as an importable package,\n",
      "      but it is not listed in the `packages` configuration of setuptools.\n",
      "  \n",
      "      'surprise.prediction_algorithms' has been automatically added to the distribution only\n",
      "      because it may contain data files, but this behavior is likely to change\n",
      "      in future versions of setuptools (and therefore is considered deprecated).\n",
      "  \n",
      "      Please make sure that 'surprise.prediction_algorithms' is included as a package by using\n",
      "      the `packages` configuration field or the proper discovery methods\n",
      "      (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "      instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "      You can read more about \"package discovery\" and \"data files\" on setuptools\n",
      "      documentation page.\n",
      "  \n",
      "  \n",
      "  !!\n",
      "  \n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-310\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-310\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: legacy-install-failure\n",
      "\n",
      "Encountered error while trying to install package.\n",
      "\n",
      "scikit-surprise\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for output from the failure.\n"
     ]
    }
   ],
   "source": [
    "#pip install scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "47ItRxlORZ9i"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Integración con Google Colab\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Machine Learning: Preprocesamiento y división de datos\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "# Análisis de datos y manipulación\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualización de datos\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilidades de fechas\n",
    "from datetime import datetime\n",
    "\n",
    "# Integración con Google Colab\n",
    "from google.colab import drive\n",
    "\n",
    "# Machine Learning: Preprocesamiento y división de datos\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Machine Learning: Métricas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Machine Learning: Modelado y validación\n",
    "from surprise import SVD, Reader, Dataset, accuracy\n",
    "from surprise.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "# Deep Learning: Construcción de modelos y capas\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout, Dot\n",
    "\n",
    "# Deep Learning: Optimización y regularización\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Deep Learning: Callbacks\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8bHcBRlQYY4h",
    "outputId": "3097562f-a506-4b75-ce75-fb852ea21586"
   },
   "outputs": [],
   "source": [
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "209f_YHQRaCi"
   },
   "source": [
    "# Paso 2: Lectura del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9vIFqmMIRaIc",
    "outputId": "2b3e766d-6d31-487d-c78c-5a881888cd57"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que tu DataFrame se llama df\n",
    "df = pd.read_csv('/content/drive/MyDrive/EspecializacionA&DS/Monografia/2doSemestre/DataFinal_Amazon.csv')  # Descomenta y proporciona la ruta si estás cargando un archivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LwnA5HnLr8r6",
    "outputId": "ce8c21ba-8e96-44a5-a56d-89bfc0c0f5f3"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5lMsQcyyS2NS"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Embedding con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQ49x7CvmOfM"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Los embeddings son representaciones vectoriales densas y de baja dimensión de ítems y/o usuarios. Estas representaciones capturan relaciones semánticas entre ítems o entre usuarios.\n",
    "\n",
    "Un método popular para generar embeddings es la factorización de matrices, como la descomposición en valores singulares (SVD). En el contexto de sistemas de recomendación, se busca factorizar la matriz de interacciones usuario-ítem en dos matrices más pequeñas (una para los usuarios y otra para los ítems) cuyo producto aproximado reproduce la matriz original lo mejor posible.\n",
    "\n",
    "Una vez que se han obtenido los embeddings, la predicción de una calificación o interacción entre un usuario e ítem se realiza tomando el producto escalar entre sus embeddings respectivos.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Simplicidad y eficiencia en términos computacionales.\n",
    "\n",
    "Puede manejar grandes conjuntos de datos debido a su naturaleza de baja dimensión.\n",
    "\n",
    "Es efectivo para capturar patrones subyacentes en los datos.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "No tiene en cuenta características adicionales de usuarios o ítems.\n",
    "\n",
    "Dificultades para manejar nuevos ítems o usuarios (problema de arranque en frío)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJ482XODiA8T"
   },
   "source": [
    "El código ahora incluye regularización en los embeddings, un callback para detener el entrenamiento si no hay mejoras (early stopping) y otro para reducir la tasa de aprendizaje si el error de validación no mejora (reduce learning rate on plateau)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oQAn6BZT8ZB"
   },
   "outputs": [],
   "source": [
    "data = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22AEmdhqU9vC"
   },
   "outputs": [],
   "source": [
    "# Crear un LabelEncoder para cada columna\n",
    "reviewerID_encoder = LabelEncoder()\n",
    "asin_encoder = LabelEncoder()\n",
    "\n",
    "# Ajustar y transformar las columnas\n",
    "data['reviewerID_encoded'] = reviewerID_encoder.fit_transform(data['reviewerID'])\n",
    "data['asin_encoded'] = asin_encoder.fit_transform(data['asin'])\n",
    "\n",
    "# Número de usuarios únicos y número de ítems únicos\n",
    "n_users = len(np.unique(data['reviewerID_encoded']))\n",
    "n_items = len(np.unique(data['asin_encoded']))\n",
    "\n",
    "# Dimensiones del embedding\n",
    "embedding_dim = 10\n",
    "\n",
    "# Entradas\n",
    "user_input = Input(shape=(1,))\n",
    "item_input = Input(shape=(1,))\n",
    "\n",
    "# Embeddings con regularización\n",
    "user_embedding = Embedding(n_users, embedding_dim, embeddings_regularizer=l2(1e-6))(user_input)\n",
    "item_embedding = Embedding(n_items, embedding_dim, embeddings_regularizer=l2(1e-6))(item_input)\n",
    "\n",
    "# Producto punto para predecir la valoración/rating\n",
    "merged = Dot(axes=2)([user_embedding, item_embedding])\n",
    "merged = Flatten()(merged)\n",
    "\n",
    "# Modelo\n",
    "model = Model(inputs=[user_input, item_input], outputs=merged)\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZN3t1PGSYEzR"
   },
   "source": [
    "verbose=0: No muestra ninguna barra de progreso ni métricas.\n",
    "\n",
    "verbose=1: Muestra una barra de progreso y actualiza las métricas después de cada lote.\n",
    "\n",
    "verbose=2: Muestra las métricas después de cada época, pero no muestra la barra de progreso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J3n02-vPg5wh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Crear los arrays con los datos\n",
    "user_ids = data['reviewerID_encoded'].values\n",
    "item_ids = data['asin_encoded'].values\n",
    "ratings = data['overall'].values\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# División del conjunto de entrenamiento en entrenamiento y validación\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NYbYJfevgXpj"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jOiSfVMfg5tw",
    "outputId": "e4487dec-c6a8-4c0f-ab19-1173f652f86e"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Entrenar el modelo con el conjunto de entrenamiento\n",
    "model.fit([user_ids_train, item_ids_train], ratings_train, validation_data=([user_ids_val, item_ids_val], ratings_val), epochs=500, batch_size=800000, verbose=1, callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RnBHBOT1oel0",
    "outputId": "bd896c81-223c-46c7-b27e-1337332945ba"
   },
   "outputs": [],
   "source": [
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SMJd_nt7g5h4",
    "outputId": "d42a83aa-d428-4fba-b224-0368db70236b"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo con el conjunto de prueba\n",
    "loss, mae = model.evaluate([user_ids_test, item_ids_test], ratings_test)\n",
    "print(f'Test Loss: {loss:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "USnKRkm0KCdW",
    "outputId": "84e6c104-f1cc-4c14-bbfb-98c5317619af"
   },
   "outputs": [],
   "source": [
    "# MSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "\n",
    "# RMSE\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "# MAPE (Mean Absolute Percentage Error)\n",
    "mape = np.mean(np.abs((y_true - predictions.flatten()) / y_true)) * 100\n",
    "print(f\"MAPE: {mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "odgbIWArkJ2y"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa con Keras:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vBK_zPK7lvti"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Estos sistemas toman características de los ítems y/o usuarios y las pasan a través de una o varias capas de neuronas para obtener una predicción.\n",
    "Las redes neuronales son capaces de capturar interacciones no lineales entre características, lo que las hace poderosas para tareas de modelado complejas.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de modelar relaciones no lineales.\n",
    "\n",
    "Flexibilidad para incorporar múltiples fuentes de datos o características.\n",
    "\n",
    "Puede manejar arranques en frío al incorporar características de nuevos ítems o usuarios.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor costo computacional en comparación con los sistemas basados únicamente en embeddings.\n",
    "\n",
    "Riesgo de sobreajuste si no se tiene un conjunto de datos lo suficientemente grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YMnns-ENh_KJ"
   },
   "source": [
    "Ahora el código incluye regularización en los embeddings, early stopping para detener el entrenamiento si el modelo deja de mejorar en el conjunto de validación, y reducción de la tasa de aprendizaje si el error en el conjunto de validación no mejora después de algunas épocas. Estas adiciones deberían ayudar a mejorar la capacidad de generalización del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZFgVZSeTkKGy"
   },
   "outputs": [],
   "source": [
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# Número de usuarios e ítems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# Hiperparámetros\n",
    "hidden_units = [128, 64, 32]\n",
    "dropout_rate = 0.2\n",
    "l2_reg = 1e-6\n",
    "\n",
    "# Arquitectura del modelo\n",
    "user_input = Input(shape=[1], name='user_input')\n",
    "item_input = Input(shape=[1], name='item_input')\n",
    "concat = Concatenate()([user_input, item_input])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)  # También puedes aplicar regularización L2 aquí\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N7uvl8VEhqdD"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "blU4Q9akkVBc"
   },
   "outputs": [],
   "source": [
    "# Datos para entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# División del conjunto de entrenamiento en entrenamiento y validación\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kGxLwUhzf32z",
    "outputId": "5860c511-44e1-477a-f298-59abaf192b61"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Entrenamiento del modelo\n",
    "history = model.fit([user_ids_train, item_ids_train], ratings_train,\n",
    "    epochs=500,validation_data=([user_ids_val, item_ids_val], ratings_val),\n",
    "    batch_size=800000,verbose=1,callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rhuwylfbf26k",
    "outputId": "adefb079-ecbb-4e53-a2ef-7ed5bab1caca"
   },
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CmYiE_PhkU66",
    "outputId": "d945c634-6546-409c-bfaa-c5d84496eaf2"
   },
   "outputs": [],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# Funciones MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la función\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WTOAZeGrujrU"
   },
   "source": [
    "# Modelo de Filtrado Colaborativo usando Red Neuronal Multicapa y Embedding con Keras:\n",
    "\n",
    "Otra enfoque popular para filtrado colaborativo es utilizar redes neuronales multicapa, que esencialmente aprenden características no lineales de los datos. Vamos a construir un modelo que fusiona los embeddings de usuarios y artículos (por ejemplo, películas) en una red neuronal densa.\n",
    "\n",
    "Este enfoque, que combina embeddings y redes neuronales multicapa, puede capturar interacciones más complejas y no lineales entre usuarios y artículos. Sin embargo, es crucial prestar atención al sobreajuste y asegurarse de que el modelo no esté simplemente memorizando los datos. Por lo tanto, es recomendable emplear técnicas de regularización, ajustar hiperparámetros y validar el rendimiento con un conjunto de datos de validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i3Rzzhtfl7nK"
   },
   "source": [
    "Funcionamiento:\n",
    "\n",
    "Combina lo mejor de ambos mundos. Primero, se utilizan embeddings para convertir ítems y usuarios en representaciones vectoriales densas. Luego, estas representaciones se pasan a través de una red neuronal para hacer la predicción.\n",
    "\n",
    "El proceso generalmente comienza con capas de embedding que convierten identificadores de usuarios e ítems en vectores. Estos vectores luego se pasan a través de capas densas para obtener la predicción final.\n",
    "\n",
    "Ventajas:\n",
    "\n",
    "Capacidad de capturar patrones subyacentes en los datos mediante embeddings y modelar interacciones no lineales mediante la red neuronal.\n",
    "\n",
    "Flexibilidad para incorporar características adicionales.\n",
    "\n",
    "Potencialmente más preciso que cualquiera de los otros dos métodos por separado.\n",
    "\n",
    "Desventajas:\n",
    "\n",
    "Mayor complejidad en el modelado y entrenamiento.\n",
    "\n",
    "Mayor costo computacional.\n",
    "\n",
    "Requiere un ajuste más cuidadoso y riesgo de sobreajuste si no se gestiona adecuadamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uY_n_5u4ymE8"
   },
   "source": [
    "Regularización L2: Agregar una regularización L2 a las capas de embedding y densas para prevenir el sobreajuste.\n",
    "\n",
    "Early Stopping: Añadir una callback de early stopping para detener el entrenamiento cuando no haya mejora en el conjunto de validación durante un cierto número de épocas.\n",
    "\n",
    "Ajuste de la tasa de aprendizaje: Utilizar la callback ReduceLROnPlateau para reducir la tasa de aprendizaje cuando no haya mejora en el conjunto de validación.\n",
    "\n",
    "Métricas adicionales: Puedes monitorizar otras métricas como el error absoluto medio (MAE) durante el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVRB5utBuqzB"
   },
   "outputs": [],
   "source": [
    "# Suponiendo que el DataFrame se llama df\n",
    "df = df\n",
    "\n",
    "# Convertir reviewerID y asin a índices numéricos secuenciales\n",
    "df['user_id'] = df['reviewerID'].astype('category').cat.codes.values\n",
    "df['item_id'] = df['asin'].astype('category').cat.codes.values\n",
    "\n",
    "# Número de usuarios e ítems\n",
    "n_users = df['user_id'].nunique()\n",
    "n_items = df['item_id'].nunique()\n",
    "\n",
    "# Hiperparámetros\n",
    "n_latent_factors = 50\n",
    "hidden_units = [128, 64]\n",
    "dropout_rate = 0.2\n",
    "l2_reg = 1e-4\n",
    "\n",
    "# Arquitectura del modelo con regularización L2\n",
    "user_embedding = Embedding(n_users, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='user_embedding')(user_input)\n",
    "item_embedding = Embedding(n_items, n_latent_factors, embeddings_regularizer=l2(l2_reg), name='item_embedding')(item_input)\n",
    "\n",
    "user_vec = Flatten(name='flatten_users')(user_embedding)\n",
    "item_vec = Flatten(name='flatten_items')(item_embedding)\n",
    "concat = Concatenate()([user_vec, item_vec])\n",
    "dense = concat\n",
    "for units in hidden_units:\n",
    "    dense = Dense(units, activation='relu', kernel_regularizer=l2(l2_reg))(dense)\n",
    "    dense = Dropout(dropout_rate)(dense)\n",
    "output = Dense(1, kernel_regularizer=l2(l2_reg))(dense)\n",
    "model = Model(inputs=[user_input, item_input], outputs=output)\n",
    "\n",
    "model.compile(optimizer=Adam(0.001), loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3I7zFMJBjTZ7"
   },
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=50, min_lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJfWiirYxHwk"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento\n",
    "user_data = df['user_id'].values\n",
    "item_data = df['item_id'].values\n",
    "rating_data = df['overall'].values\n",
    "\n",
    "# División en entrenamiento y prueba\n",
    "(user_ids_temp, user_ids_test, item_ids_temp, item_ids_test, ratings_temp, ratings_test) = train_test_split(\n",
    "    user_ids, item_ids, ratings, test_size=0.2, random_state=42)\n",
    "\n",
    "# División del conjunto de entrenamiento en entrenamiento y validación\n",
    "(user_ids_train, user_ids_val, item_ids_train, item_ids_val, ratings_train, ratings_val) = train_test_split(\n",
    "    user_ids_temp, item_ids_temp, ratings_temp, test_size=0.25, random_state=42)  # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXQNam6Bf-dT",
    "outputId": "59b3dc80-4536-42ce-b04c-19e02ea66db6"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit([user_ids_train, item_ids_train], ratings_train,\n",
    "                    epochs=500,validation_data=([user_ids_val, item_ids_val], ratings_val),\n",
    "                    batch_size=800000,verbose=1,callbacks=[early_stop, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2Off5YI9f-_T",
    "outputId": "c960e6af-49d1-4740-a170-d7c87cf63483"
   },
   "outputs": [],
   "source": [
    "# Predicciones\n",
    "y_true = ratings_test\n",
    "predictions = model.predict([user_ids_test, item_ids_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w1BY-Sd0xJtc",
    "outputId": "47f064cb-4039-4759-ee42-9e2e75673021"
   },
   "outputs": [],
   "source": [
    "# MSE y RMSE\n",
    "mse = mean_squared_error(y_true, predictions)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "# MAE\n",
    "mae = mean_absolute_error(y_true, predictions)\n",
    "\n",
    "# MAPE\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_idx = y_true != 0  # Para evitar divisiones por cero\n",
    "    return np.mean(np.abs((y_true[non_zero_idx] - y_pred[non_zero_idx]) / y_true[non_zero_idx])) * 100\n",
    "\n",
    "def compute_mape_by_batch(y_true, predictions, batch_size=50000):\n",
    "    mape_sum = 0\n",
    "    num_batches = int(np.ceil(len(y_true) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        batch_y_true = y_true[start_idx:end_idx]\n",
    "        batch_predictions = predictions[start_idx:end_idx]\n",
    "\n",
    "        mape_sum += mean_absolute_percentage_error(batch_y_true, batch_predictions)\n",
    "\n",
    "    return mape_sum / num_batches\n",
    "\n",
    "# Luego llamas a la función\n",
    "mape = compute_mape_by_batch(y_true, predictions)\n",
    "\n",
    "print(f'MSE: {mse}')\n",
    "print(f'RMSE: {rmse}')\n",
    "print(f'MAE: {mae}')\n",
    "print(f'MAPE: {mape}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YopA3D2Sw8AO"
   },
   "source": [
    "Estas métricas te ayudarán a tener una mejor idea del desempeño de tu modelo. Por ejemplo:\n",
    "\n",
    "MSE y RMSE son útiles cuando quieres penalizar grandes errores.\n",
    "\n",
    "MAE te da una idea del error medio sin considerar la dirección del error.\n",
    "\n",
    "MAPE es útil cuando quieres representar el error en términos porcentuales.\n",
    "\n",
    "Para una evaluación completa, es recomendable utilizar un conjunto de validación aparte (es decir, no solo depender del validation_split). Esto asegura que estás evaluando el desempeño en datos que el modelo nunca ha visto durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dgWsjHFlB67"
   },
   "source": [
    "En resumen, la elección del tipo de sistema de recomendación dependerá de la naturaleza del conjunto de datos, las características disponibles, las capacidades computacionales y el tipo de relaciones o interacciones que se desean capturar. A menudo, un enfoque híbrido (como el sistema que combina embeddings y redes neuronales) ofrece un buen equilibrio entre precisión y eficiencia."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
